\documentclass{article}[12pt]
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{graphicx}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newcommand{\qed}{\hfill\rule{7pt}{7pt}}
\newenvironment{proof}{\noindent{\bf Proof:}}{\qed\medskip}

\title{Confidence assignment based on game theoretic analysis}
\author{Yoav Freund}
\begin{document}

\maketitle

\newcommand{\corr}{\mbox{corr}}
\newcommand{\Exy}[1]{E_{(x_i,y_i)}\left( #1 \right)} 
\newcommand{\ones}[1]{\mathbbm{1}^{#1}}
\newcommand{\vA}{\mathbf{A}}
\newcommand{\vF}{\mathbf{F}}
\newcommand{\vI}{\mathbf{I}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vs}{\mathbf{s}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\vone}{\mathbf{1}}

\begin{itemize}
\item Suppose we have finite set of classification rules $F=(f_1,\ldots,f_m)$
that map instances $x \in X$ to $y \in \{-1,+1\}$. 
\item
Let $D$ be a fixed but unknown (to the algorithm) distribution over
$(x,y)$ pairs, where $x \in X$ and $y \in \{-1,+1\}$.
\item
We work in a transductive framework. In other words, we have $n$
unlabeled examples (instances) $x_i \in X$, denoted
$T=\{x_1,\ldots,x_n\}$.
\item
We assume that the correlation of each function $f \in F$ on the sample
$T$ is at least $(1-\epsilon)$ for some $1>\epsilon>0$.
\item
The goal of the algorithm is to find a prediction function $g:T \to
[-1,+1]$ that maximizes the worst case correlation between $g$ and the
true label.
\end{itemize}

\section{Formal Setup}
Let $z_i = \Exy{y_i | x_i}$ be the expected value of the label
associated with instance $x_i$. Clearly $-1 \leq z_i \leq 1$. For
reasons that have to do with the cannonical representation of linear
programs, we partitions each conditional probability into two positive
terms: $z_i=z_i^+ - z_i^-$, where $0 \leq z_i^+ , z_i^- \leq 1$. We
denote by $\vz$ the $2n$ dimensional column vector:
$\vz^T=(z_1^+,z_1^-,\ldots z_n^+,z_n^-)$

Similarly, we use $g_i =g_i^+ - g_i^-$ to denote the predictions made
by the algorithm. Again $0 \leq g_i^+,g_i^- \leq 1$ and 
$\vg^T \doteq (g_1^+,g_1^-,\ldots, g_n^+,g_n^-)$.

The correlation between the prediction vector $\vg$ and the
conditional probability $\vz$ is the inner product $\vz^T \vg$. The
goal of the algorithm is to maximize the correlation and the goal of
nature is to minimize it. As we (or the algorithm) are interested in
maximizing the worst case performance (over the choices of $\vz$). We
can formalize the optimization problem faced by the algorithm as
\[
\max_{\vg}\;\;\; \min_{\vz} \vz^T \vg
\]
Where $\vg,\vz \in [0,1]^n$ and $\vz$ is further constrained by the
functions in $F$.

We denote by $\vF$ the matrix the contains the prediction of
$(f_1,\ldots,f_m)$ on the instances $(x_1,\ldots,x_n)$. To match the
fact that $\vg$ and $\vz$ have two entries for each $x_i$ we similarly
double the number of rows in $\vF$, getting the following $2n \times
m$ matrix:

\begin{equation}
\vF = 
 \begin{pmatrix}
   f_1(x_1) &  f_2(x_1) & \cdots &  f_m(x_1) \\
  -f_1(x_1) & -f_2(x_1) & \cdots & -f_m(x_1) \\
   f_1(x_2) &  f_2(x_2) & \cdots &  f_m(x_2) \\
   \vdots   & \vdots    & \ddots &  \vdots  \\
   f_1(x_n)  &  f_2(x_n)  & \cdots &   f_m(x_n) \\
  -f_1(x_n)  & -f_2(x_n)  & \cdots &  -f_m(x_n) 
 \end{pmatrix}
\end{equation}

We can represent the $m$ constraints on $\vz$ defined by
$(f_1,\ldots,f_m)$, as
\[
\vz^T \vF \geq (1-\epsilon)n \ones{m}
\]
Where $\ones{m}$ denote a row vector of length $m$ all of which
entries are equal to $1$.

We represent the constraints $\vz \leq \ones{2n}$ as a larger-or-equal
constraint
\[
\vz^T (-\vI) \geq -\ones{2n}
\] 

\newpage

\section{The optimization problem}

We can rewrite the optimization problem in matrix notation as follows.
We use $\vb,\vc,\ldots$ to denote column vectors.  We use the notation
$(\vb^T,\vc^T)$ to denote the row vector which is the concatenation of
$\vb^T$ and $\vc^T$.

The problem is

\begin{eqnarray}
\mbox{Find: }&\max_{\vg} \;\; \min_{\vz} \vz^T \vg \label{max-min-prog}\\
\mbox{Such That: }& \vz^T \vA \geq \vc^T \mbox{ and } \vz \geq \vzero
\notag \\
& -\vg \geq -\ones{2n} \mbox{ and } \vg \geq \vzero \notag
\end{eqnarray}

Where 
\begin{equation}
\vc^T = (n (1-\epsilon) \ones{m}, -\ones{2n})
\end{equation}
and $\vA$ is the  $2n \times (m+2n)$ matrix: 
\begin{equation}
\vA = \left( \vF, -\vI \right) =
 \begin{pmatrix}
   f_1(x_1) &  f_2(x_1) & \cdots &  f_m(x_1) & -1 & 0 & 0 & \cdots & 0
   & 0 \\
  -f_1(x_1) & -f_2(x_1) & \cdots & -f_m(x_1) & 0 & -1 & 0 & \cdots & 0
  & 0 \\
   f_1(x_2) &  f_2(x_2) & \cdots &  f_m(x_2) & 0 & 0 & -1 & \cdots & 0
   & 0 \\
  \vdots   & \vdots    & \ddots &  \vdots  & \vdots & \vdots & \vdots
  & \ddots & \vdots & \vdots \\
   f_1(x_n)  &  f_2(x_n)  & \cdots &   f_m(x_n) & 0  & 0 & 0 & \cdots
   & -1 & 0 \\
  -f_1(x_n)  & -f_2(x_n)  & \cdots &  -f_m(x_n) & 0  & 0 & 0 & \cdots
  & 0 & -1 \\
 \end{pmatrix}
\end{equation}

\subsection{Transforming problem into an LP}

Suppose we fix $\vg$, the internal maximization problem is an LP:
\begin{eqnarray}
\mbox{Find: }&\min_{\vz} \vz^T \vg \label{min-LP}\\
\mbox{Such That: }& \vz^T \vA \geq \vc^T \mbox{ and } \vz \geq \vzero \notag
\end{eqnarray}

We can write the maximization LP that is the dual to this minimization LP:
\begin{eqnarray}
\mbox{Find: }& \max_\vv \vc^T \vv \label{max-LP}\\
\mbox{Such That: }& \vA \vv \leq \vg \mbox{ and } \vv \geq \vzero \notag
\end{eqnarray}

We partition the vector $\vv$ into two parts: $\vv^T = (\vr^T,\vs^T)$,
where $\vr$ is the $m$ dimensional vector corresponding to the $m$
functions and $\vs$ is the $2n$ vector corresponding to the $n$ data
points.  Using this notation we can rewrite the dual LP as follows:
\begin{eqnarray}
\mbox{Maximize: }& (1-\epsilon)n \|\vr\|_1  - \| \vs \|_1 \label{expanded-max-LP}\\
\mbox{Such That: }& \vA \vr  - \vs \leq \vg \notag \\
\mbox{ and } & \vr \geq \vzero \mbox{ and } \vs \geq \vzero \notag
\end{eqnarray}
We can substitute the program~(\ref{expanded-max-LP})
into~(\ref{max-min-prog}) and get the following maximization LP:
\begin{eqnarray}
\mbox{Maximize: }& (1-\epsilon)n \|\vr\|_1  - \| \vs \|_1 \label{Combined-LP-1}\\
\mbox{Such That: }& \vA \vr  - \vs \leq \vg  \label{Combined-LP-2}\\
\mbox{ and } & \vr \geq \vzero \mbox{ and } \vs \geq \vzero \label{Combined-LP-3} \\
\mbox{ and } &  -\vg \geq -\ones{2n} \mbox{ and } \vg \geq \vzero \label{Combined-LP-4}
\end{eqnarray}

To maximize (\ref{Combined-LP-1}) when $\vr$ is fixed, we want to
minimize $\|\vs\|_1$. The only constraint on $\vs$ (other than $\vs
\geq \vzero$) is in~(\ref{Combined-LP-2}), and the only constraints on
$\vg$ are in~(\ref{Combined-LP-4}).

We can therefor simplify the linear program to the following form:
\begin{eqnarray}
\mbox{Maximize: }& (1-\epsilon)n \|\vr\|_1  - \| \vs \|_1 \label{condensed-LP-1}\\
\mbox{Such That: }& \vA \vr  - \vs \leq \ones{2n}  \label{Condensed-LP-2}\\
\mbox{ and } & \vr \geq \vzero \mbox{ and } \vs \geq \vzero \label{Condensed-LP-3} \\
\end{eqnarray}

For each $1 \leq i \leq 2n$ we have that
\[
s_i = \max \left(0,(\vA \vr)_i -1 \right)
\]

From the definition of $\vA$ we get that fo all $1 \leq i \leq n$
\begin{eqnarray*}
s_{2i-1} &=& \max \left(0,\sum_{j=1}^m f_j(x_i) r_j -1 \right) \\
s_{2i} &=& \max \left(0,-\sum_{j=1}^m f_j(x_i) r_j -1 \right)
\end{eqnarray*}
which implies 
\[
s_{2i-1}+s_{2i} = \max \left(0,\left| \sum_{j=1}^m f_j(x_i) r_j \right| -1 \right)
\]
\end{document}

