\documentclass{article}[12pt]
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{graphicx}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newcommand{\qed}{\hfill\rule{7pt}{7pt}}
\newenvironment{proof}{\noindent{\bf Proof:}}{\qed\medskip}

\title{Confidence assignment based on game theoretic analysis}
\author{Yoav Freund}
\begin{document}

\maketitle

\newcommand{\corr}{\mbox{corr}}
\newcommand{\Exy}[1]{E_{(x_i,y_i)}\left( #1 \right)} 
\newcommand{\ones}[1]{\mathbbm{1}^{#1}}
\newcommand{\vA}{\mathbf{A}}
\newcommand{\vF}{\mathbf{F}}
\newcommand{\vI}{\mathbf{I}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vd}{\mathbf{d}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vs}{\mathbf{s}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\vone}{\mathbf{1}}

\begin{itemize}
\item Suppose we have finite set of classification rules $F=(f_1,\ldots,f_m)$
that map instances $x \in X$ to $y \in \{-1,+1\}$. 
\item
Let $D$ be a fixed but unknown (to the algorithm) distribution over
$(x,y)$ pairs, where $x \in X$ and $y \in \{-1,+1\}$.
\item
We work in a transductive framework. In other words, we have $n$
unlabeled examples (instances) $x_i \in X$, denoted
$T=\{x_1,\ldots,x_n\}$.
\item
We assume that some of the functions in $F$ have non-zero
correlation with the true distribution over the labels, we denote
these correlations (or a lower bound on the correlation) by
\[
\forall 1 \leq i \leq m\;\; c_i \leq \frac{1}{n} \sum_{x\in T} E_{y \sim D(x)}\left[ y\cdot f_i(x)\right]
\]
\item
The goal of the algorithm is to find a prediction function 
$g:T \to [-1,+1]$ that maximizes the worst case correlation between $g$ and the
true label.
\end{itemize}

\section{Formal Setup}
Let $z_i = \Exy{y_i | x_i}$ be the expected value of the label
associated with instance $x_i$. Clearly $-1 \leq z_i \leq 1$. For
reasons that have to do with the cannonical representation of linear
programs, we partitions each conditional probability into two positive
terms: $z_i=z_i^+ - z_i^-$, where $0 \leq z_i^+ , z_i^- \leq 1$. We
denote by $\vz$ the $2n$ dimensional column vector:
$\vz^T=(z_1^+,z_1^-,\ldots z_n^+,z_n^-)$

Similarly, we use $g_i =g_i^+ - g_i^-$ to denote the predictions made
by the algorithm. Again $0 \leq g_i^+,g_i^- \leq 1$ and 
$\vg^T \doteq (g_1^+,g_1^-,\ldots, g_n^+,g_n^-)$.

Finally we use the vector $\vc$ to denote the column vector of the
function correlations $c_i$.

The correlation between the prediction vector $\vg$ and the
conditional probability $\vz$ is the inner product $\vz^T \vg$. The
goal of the algorithm is to maximize the correlation and the goal of
nature is to minimize it. As we (or the algorithm) are interested in
maximizing the worst case performance (over the choices of $\vz$). We
can formalize the optimization problem faced by the algorithm as
\[
\max_{\vg}\;\;\; \min_{\vz} \vz^T \vg
\]
Where $\vg,\vz \in [0,1]^n$ and $\vz$ is further constrained by the
functions in $F$.

We denote by $\vF$ the matrix the contains the prediction of
$(f_1,\ldots,f_m)$ on the instances $(x_1,\ldots,x_n)$. To match the
fact that $\vg$ and $\vz$ have two entries for each $x_i$ we similarly
double the number of rows in $\vF$, getting the following $2n \times
m$ matrix:

\begin{equation}
\vF = 
 \begin{pmatrix}
   f_1(x_1) &  f_2(x_1) & \cdots &  f_m(x_1) \\
  -f_1(x_1) & -f_2(x_1) & \cdots & -f_m(x_1) \\
   f_1(x_2) &  f_2(x_2) & \cdots &  f_m(x_2) \\
   \vdots   & \vdots    & \ddots &  \vdots  \\
   f_1(x_n)  &  f_2(x_n)  & \cdots &   f_m(x_n) \\
  -f_1(x_n)  & -f_2(x_n)  & \cdots &  -f_m(x_n) 
 \end{pmatrix}
\end{equation}

We can represent the $m$ constraints on $\vz$ defined by
$(f_1,\ldots,f_m)$, as
\[
\vz^T \vF \geq n \vc
\]

We represent the constraints $\vz \leq \ones{2n}$ as a larger-or-equal
constraint
\[
\vz^T (-\vI) \geq -\ones{2n}
\] 
Where $\ones{2n}$ denote a row vector of length $2n$ all of which
entries are equal to $1$.

\newpage

\section{The optimization problem}

We can rewrite the optimization problem in matrix notation as follows.
We use $\vb,\vc,\ldots$ to denote column vectors.  We use the notation
$(\vb^T,\vc^T)$ to denote the row vector which is the concatenation of
$\vb^T$ and $\vc^T$.

The problem is

\begin{eqnarray}
\mbox{Find: }&\max_{\vg} \;\; \min_{\vz} \vz^T \vg \label{max-min-prog}\\
\mbox{Such That: }& \vz^T \vA \geq \vd^T \mbox{ and } \vz \geq \vzero
\notag \\
& -\vg \geq -\ones{2n} \mbox{ and } \vg \geq \vzero \notag
\end{eqnarray}

Where 
\begin{equation}
\vd^T = (n \vc^T, -\ones{2n})
\end{equation}
and $\vA$ is the  $2n \times (m+2n)$ matrix: 
\begin{equation}
\vA = \left( \vF, -\vI \right) =
 \begin{pmatrix}
   f_1(x_1) &  f_2(x_1) & \cdots &  f_m(x_1) & -1 & 0 & 0 & \cdots & 0
   & 0 \\
  -f_1(x_1) & -f_2(x_1) & \cdots & -f_m(x_1) & 0 & -1 & 0 & \cdots & 0
  & 0 \\
   f_1(x_2) &  f_2(x_2) & \cdots &  f_m(x_2) & 0 & 0 & -1 & \cdots & 0
   & 0 \\
  \vdots   & \vdots    & \ddots &  \vdots  & \vdots & \vdots & \vdots
  & \ddots & \vdots & \vdots \\
   f_1(x_n)  &  f_2(x_n)  & \cdots &   f_m(x_n) & 0  & 0 & 0 & \cdots
   & -1 & 0 \\
  -f_1(x_n)  & -f_2(x_n)  & \cdots &  -f_m(x_n) & 0  & 0 & 0 & \cdots
  & 0 & -1 \\
 \end{pmatrix}
\end{equation}

\subsection{Transforming the optimization problem into an LP}

Suppose we fix $\vg$, the internal maximization problem is an LP:
\begin{eqnarray}
\mbox{Find: }&\min_{\vz} \vz^T \vg \label{min-LP}\\
\mbox{Such That: }& \vz^T \vA \geq \vd^T \mbox{ and } \vz \geq \vzero \notag
\end{eqnarray}

We can write the maximization LP that is the dual to this minimization LP:
\begin{eqnarray}
\mbox{Find: }& \max_\vv \vd^T \vv \label{max-LP}\\
\mbox{Such That: }& \vA \vv \leq \vg \mbox{ and } \vv \geq \vzero \notag
\end{eqnarray}

Plugging~(\ref{max-LP}) back into~(\ref{max-min-prog}) we get
\begin{eqnarray}
\mbox{Find: }& \max_\vv \vd^T \vv \label{combined1}\\
\mbox{Such That: }& \vA \vv \leq \vg \mbox{ and } \vv \geq \vzero
\notag \\
\mbox{And:}& -\vg \geq -\ones{2n} \mbox{ and } \vg \geq \vzero \notag
\end{eqnarray}
Which is maximized when $\vg = \ones{2n}$ so we can substitute for
$\vg$ and get:
\begin{eqnarray}
\mbox{Find: }& \max_\vv \vd^T \vv \label{combined2}\\
\mbox{Such That: }& \vA \vv \leq \ones{2n} \mbox{ and } \vv \geq \vzero \notag
\end{eqnarray}

\section{Solving a simple case numerically}

In order to gain some intuition about this problem we start by
considering a very simple setup: threshold functions on the line.
We define 
\newcommand{\sign}{\mbox{sign}}
\[
f_{\theta}(x) = \sign(x-\theta)
\]
To start, consider a function set that contains a single function
$F=\{f_0\}$, and suppose that the .

\section{Towards an interpretation}
We partition the vector $\vv$ into two parts: $\vv^T = (\vr^T,\vs^T)$,
where $\vr$ is the $m$ dimensional vector corresponding to the $m$
functions and $\vs$ is the $2n$ vector corresponding to the $n$ data
points.  Using this notation we can rewrite the dual LP as follows:
\begin{eqnarray}
\mbox{Maximize: }& (n \vc^T \vr  - \| \vs \|_1 \label{expanded-max-LP}\\
\mbox{Such That: }& \vF \vr  - \vs \leq \vg \notag \\
\mbox{ and } & \vr \geq \vzero \mbox{ and } \vs \geq \vzero \notag
\end{eqnarray}
We can substitute the program~(\ref{expanded-max-LP})
into~(\ref{max-min-prog}) and get the following maximization LP:
\begin{eqnarray}
\mbox{Maximize: }& n \vc^T \vr  - \| \vs \|_1 \label{Combined-LP-1}\\
\mbox{Such That: }& \vF \vr  - \vs \leq \vg  \label{Combined-LP-2}\\
\mbox{ and } & \vr \geq \vzero \mbox{ and } \vs \geq \vzero \label{Combined-LP-3} \\
\mbox{ and } &  -\vg \geq -\ones{2n} \mbox{ and } \vg \geq \vzero \label{Combined-LP-4}
\end{eqnarray}

To maximize (\ref{Combined-LP-1}) when $\vr$ is fixed, we want to
minimize $\|\vs\|_1$. The only constraint on $\vs$ (other than $\vs
\geq \vzero$) is in~(\ref{Combined-LP-2}), and the only constraints on
$\vg$ are in~(\ref{Combined-LP-4}).

We can therefor simplify the linear program to the following form:
\begin{eqnarray}
\mbox{Maximize: }& n \vc^T \vr  - \| \vs \|_1 \label{condensed-LP-1}\\
\mbox{Such That: }& \vF \vr  - \vs \leq \ones{2n}  \label{Condensed-LP-2}\\
\mbox{ and } & \vr \geq \vzero \mbox{ and } \vs \geq \vzero \label{Condensed-LP-3} \\
\end{eqnarray}

For each $1 \leq i \leq 2n$ we have that
\[
s_i = \max \left(0,(\vF \vr)_i -1 \right)
\]

From the definition of $\vF$ we get that fo all $1 \leq i \leq n$
\begin{eqnarray*}
s_{2i-1} &=& \max \left(0,\sum_{j=1}^m f_j(x_i) r_j -1 \right) \\
s_{2i} &=& \max \left(0,-\sum_{j=1}^m f_j(x_i) r_j -1 \right)
\end{eqnarray*}
which implies 
\[
s_{2i-1}+s_{2i} = \max \left(0,\left| \sum_{j=1}^m f_j(x_i) r_j \right| -1 \right)
\]
\end{document}

