\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}

\usepackage{geometry}
\geometry{a4paper,
  total={170mm,220mm},
  marginparwidth=80mm,
left=5mm,
right=85mm,
top=20mm,
}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}


\title{A simple agressive active learning algorithm}
\author{Akshay Balsubramani, Yoav Freund, Shay Moran}
\date{December 2016}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}[theorem]{assumption}
\newtheorem{definition}[theorem]{Definition}

\newcommand{\err}{\mbox{err}}
\newcommand{\X}{{\cal X}}
\newcommand{\Y}{{\cal Y}}
\renewcommand{\L}{{\cal L}}
\newcommand{\x}{\vec{x}}
\newcommand{\y}{\vec{y}}
\newcommand{\vv}{\vec{v}}
\newcommand{\cc}{\vec{c}}

\newcommand{\K}{{\cal K}}
\newcommand{\restrictedto}{\triangleright}
\renewcommand{\SS}{{\cal S}}
\newcommand{\outcome}{z}
\newcommand{\empoutcome}{\hat{\outcome}}
\newcommand{\polarity}{p}
\newcommand{\diameter}{D}
\newcommand{\weight}{\omega}

\newcommand{\bias}{\text{bias}}
\newcommand{\empbias}{\widehat{\text{bias}}}
\newcommand{\sign}{\text{sign}}
\newcommand{\bayes}{\beta}
\newcommand{\new}[1]{\textcolor{red}{#1}}

\newcommand{\comment}[3]{\marginpar{\textcolor{#2}{#1: #3}}}
%\newcommand{\comment}[3]{}
\newcommand{\shay}[1]{\comment{Shay}{red}{#1}}
\newcommand{\yoav}[1]{\comment{Yoav}{blue}{#1}}
\newcommand{\akshay}[1]{\comment{Akhay}{magenta}{#1}}

\begin{document}

\maketitle

\begin{abstract}
We describe a partition based learning algorithm for $[0,1]^n$ which
achieves Bayes optimal performance in the limit. We show
a rate of covergence similar to that shown by Dasgupta and Chaudhuri.

We describe an active learning version of the above algorithm which
achieves exponential speedup in some natural situations.

\end{abstract}

\section{Problem setup}

The input space $\X$ is a finite set of points in $[0,1]^d$.
The label of each point is drawn according to a conditional
distribution over $\{-1,+1\}$. We denote the conditional probability
of the label by $\outcome(\x) \doteq P[Y=1 | X=\x]$.

The goal is to achieve error close to the Bayes optimal rule, which is
\[
\beta(\x) =
\begin{cases}
 0 & \outcome(\x)<1/2\\
 1/2 & \outcome(\x)=1/2\\
 1 & \outcome(\x)>1/2
\end{cases}
\]

\section{A passive algorithm}

The algorithm is based a hierarchical partitioning of $[0,1]^d$.
The partition at level $i = 0,1,2,\ldots$ consists of $2^{id}$ cubes
of size $2^{-i}$. The cubes are indexed by a sequence of $d$
integers between $0$ and $2^i-1$ such that
\[
Q(j_1,\ldots,j_d) = \left\{\x\;\; \text{such that}\;\;
\forall k\in\{1,\ldots,d\},  \frac{j_k}{2^i} \leq x_k < \frac{j_k+1}{2^i} \right\}
\]

The passive algorithm recieves as input $n$ labeled examples. It
computes a prediction for each point that is in the set
$0,1/2,1$. This is done as follows:
\begin{enumerate}
\item For each cube in the hierarchy which contains at least one
  labeled example, the algorithm computes a confidence interval for
  the conditional probability in that interval. Denote the cube by
  $a$, denote the conditional probability by $\empbias(a)$ and deonte
  the number of points that fall inside $a$ by $m(a)$. Then the
  confidence interval is defined to be 
  \[
    [l(a),u(a)] = \left[ \empbias(a)-\Delta(a), \empbias(a) + \Delta(a) \right]
    \cap [0,1]
  \]
  Where
  \[
    \Delta(a) = \sqrt{\frac{\ln 2/\delta+(d+1) \ln m(a)}{m(a)/2}}
    \]
\item For each cube $a$ we deifine the polarity of $a$ to be
  \[
  \polarity(a) =
  \begin{cases}
    0   & \text{if } u(a)<1/2\\
    1   & \text{if } l(a)>1/2\\
    1/2 & \text{oterwise}
  \end{cases}
  \]
\item To compute the prediction on a point $\x$ the algorithm
  considers all cubes that contain the point and whose polarity is not
  $1/2$. If this is an empty set, then the prediction is $1/2$. If it
  is not an empty set, then the prediction is the polarity of the
  smallest cube containing $\x$.
          

\end{enumerate}

\subsection{Smoothness assumptions}

Following~\cite{ChaudhuriDas2014} we partition the input space into
interior sets and boundary sets. We use the same parameters
$p,\Delta \geq 0$ to quantify the partition.

Unlike~\cite{ChaudhuriDas2014} our algorithm makes a prediction on
$\x$ by using the confidence interval for the cube $Q$ that contains
$\x$. We therefor define the interior sets diffferently.

\newcommand{\Set}{{\cal X}}

We define $\Set_{p,\delta}^+$ to be a set of points on which the
prediction is $1$ with high probability for cubes of probability
at most $p$ i.e.
\[
\Set_{p,\Delta}^+ = \left\{ \x\;\; \left| \;\; \forall Q,\; \mbox{s.t.} \x \in
Q, P(Q) \leq p,\;\; \bias(Q) \geq \frac{1}{2}+\Delta \right.\right\}
\]
Similarly
\[
\Set_{p,\Delta}^- = \left\{ \x\;\; \left| \;\; \forall Q,\; \mbox{s.t.} \x \in
Q, P(Q) \leq p,\;\; \bias(Q) \leq \frac{1}{2}-\Delta \right.\right\}
\]
the remaining part of $[0,1]^n$ is the $(p,\Delta)$-effective boundary
\[
\partial_{p,\Delta} = [0,1]^n - (\Set_{p,\Delta}^+ \cup \Set_{p,\Delta}^-)
\]


We can prove something similar to theorem 5 in~\cite{ChaudhuriDas2014}.

\section{Active Learning}

The idea of active learning is to partition the effective boundary
into two parts which we call the known unknown (KU) and the unknown
unknown (UU). KU corresponds to the boundaries between detected
regions of different labels, while UU corresponds to a boundary
surrounding a regions that is yet to be detected. We use uniform
sampling to detect UU boundaries, we use sampling of the {\em border
  set} to shrink the uncertainty around KU.

Recall the last step of the passive algorithm:

\begin{itemize}
  \item To compute the prediction on a point $\x$ the algorithm
  considers all cubes that contain the point and whose polarity is not
  $1/2$. If this is an empty set, then the prediction is $1/2$. If it
  is not an empty set, then the prediction is the polarity of the
  smallest cube containing $\x$.
\end{itemize}

We now define the border. Let $\epsilon>0$ to be an infinitesimally small
number. We say that $\x,\y$ are border points if the predictions on
them is different ($0,1/2,1$) and the distance between them is smaller
than $\epsilon$.

We define the border cubes to include all of the cubes that ``are
responsible'' for the prediction on a border point. The border set is
the union of all of the border cubes.

The active algorithm works in phases. Before each phase we query the
label on $2n$ instances. $n$ instances are chosen uniformly at random
from the whole set of unlabeled examples, and another $n$ instances
are chosen uniformly at random from the border set.

What I think we can show: suppose that the condition probability of
the label is either $1/4$ or $3/4$ and that the boundary
between the regions is smooth (has Hausdorff dimension $n-1$) then the
query complexity of this algorithm is logarithmic in the regret (error
is bayes (=1/4) + regret)

\end{document}
