\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}

\title{Active learning using muffler}
\author{Akshay Balsubramani, Yoav Freund, Shay Moran}
\date{November 2016}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}[theorem]{assumption}
\newtheorem{definition}[theorem]{Definition}

\newcommand{\err}{\mbox{err}}
\newcommand{\X}{{\cal X}}
\newcommand{\Y}{{\cal Y}}
\newcommand{\x}{\vec{x}}
\newcommand{\y}{\vec{y}}
\newcommand{\vv}{\vec{v}}
\newcommand{\cc}{\vec{c}}

\newcommand{\I}{{\cal I}}
\newcommand{\M}{{\cal M}}

\newcommand{\bias}{\text{bias}}
\newcommand{\sign}{\text{sign}}


\newcommand{\shay}[1]{\textcolor{red}{Shay: #1}}
\newcommand{\yoav}[1]{\textcolor{blue}{Yoav: #1}}
\newcommand{\akshay}[1]{\textcolor{green}{Akshay: #1}}

\begin{document}

\maketitle

\section{Introduction}

As a first step towards using Muffler for active learning, we describe
a setup in which Muffler converges to the Bayes optimal rule.

We operate in a restricted context which emulates the kNN 
convergence rate analysis of Chaudhuri and Dasgupta.

\section{Problem setup}

The input space $\X$ is a finite set of points in $R^d$.
The label of each point is drawn according to a conditional
distribution over $\{-1,+1\}$. Instead of the conditional
distribution, we use the conditional expectation
$\rho(\x) \doteq E[Y | X=\x]$.

The rules that we use are ``specialists'' that are balls of the form:
\[
B_{r,\cc}(\x) =
\begin{cases}
 1 & \text{if } \| \cc- \x \| \leq r \\
 0 & \text{otherwise }
\end{cases}
\]

If $B_{r,\cc}(\x) =1$ for some $\x \in \X$ we say that $B_{r,\cc}$
{\em covers} $\x$. We call the set of all $\x \in \X$ that are covered
by $B_{r,\cc}$ the {\em support} of $B_{r,\cc}$, we denote the size of
the support of $B$ by $|B|$.

We define the {\em bias} of a ball $B$ as the average of the
conditional expectations over the support of $B$.
$$
\bias(B) \doteq \frac{1}{|B|} \sum_{\vv \in B} \rho(\vv)
$$

We use the following definitions.

The first definition characterizes the size of the neighborhood of a
point $\x$ that is consistent with the Bayes prediction on $\x$
\begin{definition}[Neighborhood support]
Let $p(\x)$ be the Bayes optimal prediction at the point $\x$. In
other words $p(\x) = \sign(\rho(\x))$. We define the $k$ neighborhood
support  of the  point $\x$ to be the minimal value of $p(\x)\bias(B)$
over all balls $B$ such that $\x \in B$ and $|B|\leq k$. We use the
notation $\gamma_k(\x)$ to denote this quantity.
\end{definition}

This definition is what we need for our analysis. We will show a
non-trivial lower bound on $\gamma_k(\x)$ for the case where the
points $\x$ are drawn from a density distribution over $R^n$ with a
Lipshitz condition and the conditional probability $\rho(\x)$ also
satisfies a lipshitz condition.


The second definition is a technical condition that we need for our
proofs that (I believe) holds for $\X$ which is drawn from a density
distribution over $R^n$
\begin{definition}[Decreasing support]
The set $\X$ provides decreasing support for the set of balls if for
any $\x \in \X$ and for any ball $B$ that covers $\x$ there exists
another ball $B' \subset B$ that also covers $\x$ such that $|B'| = |B|-1$.
\end{definition}

\iffalse
We assume that $\rho$ is Lipschitz
with parameter $\alpha$, i.e. for any two vectors $\vec{x}\neq\vec{y}$.

$$\frac{\rho(\x)-\rho(\y)}{\|\x-\y\|_2} \leq \alpha$$
\fi

\section{Passive learning}

We first analyze the convergence rate of a passive learning algorithm.
This algorithm is Muffler using balls as specialists. More
specifically, we assume that the input space $\X \in R^n$ is finite
and that the training set is generated by repeatedly drawing a point
$\x$ from $\X$ uniformly at random and labeling the point according
to the binary distribution of $y$ conditioned on $\X$. Note that two
examples with $\x$ as the input can have different labels.

It is trivial that this algorithm will asymptotically converge to the
Bayes optimal rule. The rate of convergence at a point $\x$ depends on 
the rate at which $\gamma_k(\x)$ increases as $k$ decreases.

\iffalse
Our analysis consists of two parts:
\begin{itemize}
\item {\bf Sampling and convergence:} In this part we use large
  deviation bounds to compute a confidence interval for the bias of
  each ball. The length of the confidence interval is not uniform and
  is a function of the number of examples
  
\end{itemize}
\fi

\section{Active Learning}

In this section we provide a variant of the Muffer algorithm that
requires the labeling of far fewer examples than passive sampling.

\yoav{Algorithm to be described here}


The analysis of the algorithm consists of two steps.
First, we analyze an idealized algorithm $\I$
which operates under the assumption that exact conditional
probabilities are available. Then, in the second step, we remove the
assumption of exact conditional probabilities, replace it with
estimates based on a sample, and analyze the sample based algorithm $\M$.

We define the set of all balls whose support size is at most $k$ by $S_k$.

The parametrized algorithm $I(k)$ uses $S_k$ as the set of
specialists.  It receives as input the exact bias $\bias(B)$ for all $B \in S_r$.

The algorithm solves the min-max weighting of the rules $\sigma^*(r)$.

\begin{lemma}
  Suppse $B_1$ and $B_2$ are two balls that cover a point $\x$,
  suppose also that $B_2 \subset B_1$ and that $|B_2| < |B_1|$, then
  removing $B_1$ from the set of constraints does not change the
  optimal prediction on $\x$.
\end{lemma}

\end{document}
