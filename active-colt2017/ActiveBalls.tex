\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}


\title{Active learning using muffler}
\author{Akshay Balsubramani, Yoav Freund, Shay Moran}
\date{November 2016}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}[theorem]{assumption}
\newtheorem{definition}[theorem]{Definition}

\newcommand{\err}{\mbox{err}}
\newcommand{\X}{{\cal X}}
\newcommand{\Y}{{\cal Y}}
\newcommand{\x}{\vec{x}}
\newcommand{\y}{\vec{y}}
\newcommand{\vv}{\vec{v}}
\newcommand{\cc}{\vec{c}}

\newcommand{\K}{{\cal K}}
\newcommand{\restrictedto}{\triangleright}
\renewcommand{\SS}{{\cal S}}
\newcommand{\emprho}{\hat{\rho}}

\newcommand{\bias}{\text{bias}}
\newcommand{\sign}{\text{sign}}

\newcommand{\shay}[1]{\textcolor{red}{Shay: #1}}
\newcommand{\yoav}[1]{\textcolor{blue}{Yoav: #1}}
\newcommand{\akshay}[1]{\textcolor{magenta}{Akshay: #1}}

\begin{document}

\maketitle

\section{Introduction}

As a first step towards using Muffler for active learning, we describe
a setup in which Muffler converges to the Bayes optimal rule.

We operate in a restricted context which emulates the kNN 
convergence rate analysis of Chaudhuri and Dasgupta.

\section{Problem setup}

The input space $\X$ is a finite set of points in $R^d$.
The label of each point is drawn according to a conditional
distribution over $\{-1,+1\}$. Instead of the conditional
distribution, we use the conditional expectation
$\rho(\x) \doteq E[Y | X=\x]$.

The rules that we use are ``specialists'' that are balls of the form:
\[
B_{r,\cc}(\x) =
\begin{cases}
 1 & \text{if } \| \cc- \x \| \leq r \\
 0 & \text{otherwise }
\end{cases}
\]

If $B_{r,\cc}(\x) =1$ for some $\x \in \X$ we say that $B_{r,\cc}$
{\em covers} $\x$. We call the set of all $\x \in \X$ that are covered
by $B_{r,\cc}$ the {\em support} of $B_{r,\cc}$, we denote the size of
the support of $B$ by $|B|$.

We define the {\em bias} of a ball $B$ as the average of the
conditional expectations over the support of $B$.
$$
\bias(B) \doteq \frac{1}{|B|} \sum_{\vv \in B} \rho(\vv)
$$

We use the following definitions.

The first definition characterizes the size of the neighborhood of a
point $\x$ that is consistent with the Bayes prediction on $\x$
\begin{definition}[Neighborhood support]
Let $p(\x)$ be the Bayes optimal prediction at the point $\x$. In
other words $p(\x) = \sign(\rho(\x))$. We define the $k$ neighborhood
support  of the  point $\x$ to be the minimal value of $p(\x)\bias(B)$
over all balls $B$ such that $\x \in B$ and $|B|\leq k$. We use the
notation $\gamma_k(\x)$ to denote this quantity.
\end{definition}

This definition is what we need for our analysis. We will show a
non-trivial lower bound on $\gamma_k(\x)$ for the case where the
points $\x$ are drawn from a density distribution over $R^n$ with a
Lipshitz condition and the conditional probability $\rho(\x)$ also
satisfies a lipshitz condition.


The second definition is a technical condition that we need for our
proofs that (I believe) holds for $\X$ which is drawn from a density
distribution over $R^n$
\begin{definition}[Decreasing support]
The set $\X$ provides decreasing support for the set of balls if for
any $\x \in \X$ and for any ball $B$ that covers $\x$ there exists
another ball $B' \subset B$ that also covers $\x$ such that $|B'| = |B|-1$.
\end{definition}

\shay{I think the above definition is satisfied by every finite set:
Let $\x \in \X$, and let $B$ be a ball covering $\x$. 
It seems possible to modify the ball ever-so-slightly, 
such that (i) its intersection with $\X$ is unchanged,  and (ii) the 
distances of the points in $\X$ from its center are distinct.
Now, $B'$ is obtained by decreasing the radius of $B$ until
the furthest point in it from its center is removed.  Does it make sense?}
\yoav{Yes, sounds good, the only small refinement is that you need to
  show that there is always a way to do this which does not remove the
  special point $\x$.}
\iffalse
We assume that $\rho$ is Lipschitz
with parameter $\alpha$, i.e. for any two vectors $\vec{x}\neq\vec{y}$.

$$\frac{\rho(\x)-\rho(\y)}{\|\x-\y\|_2} \leq \alpha$$
\fi

\section{Passive learning}

We first analyze the convergence rate of a passive learning algorithm.
This algorithm is Muffler using balls as specialists. More
specifically, we assume that the input space $\X \in R^n$ is finite
and that the training set is generated by repeatedly drawing a point
$\x$ from $\X$ uniformly at random and labeling the point according
to the binary distribution of $y$ conditioned on $\X$. Note that two
examples with $\x$ as the input can have different labels.

It is trivial that this algorithm will asymptotically converge to the
Bayes optimal rule. The rate of convergence at a point $\x$ depends on 
the rate at which $\gamma_k(\x)$ increases as $k$ decreases.

\iffalse
Our analysis consists of two parts:
\begin{itemize}
\item {\bf Sampling and convergence:} In this part we use large
  deviation bounds to compute a confidence interval for the bias of
  each ball. The length of the confidence interval is not uniform and
  is a function of the number of examples
  
\end{itemize}
\fi

\section{Active Learning}

In this section we provide a variant of the Muffler algorithm that
requires the labeling of far fewer examples than passive sampling.

We review and define some notation we use to describe the active learning algorithm: 

\begin{itemize}
\item $\X$ is the set of unlabeled data which defines the
  universe for the problem.
\item $\K_i \subset \X$ is the set of ``known unknowns'' after iteration $i$, in
  other words the set of ``unsafe'' points. $\K_0=\emptyset$.
\item For any set $A\subseteq \X$, denote $Q(A,n)$ to be a set of
  $n$ labeled examples $(\x,y)$ generated as follows. $\x$ chosen
  independently at random from the uniform distibution over
  $A$. The label $y$ is then chosen independently at random according
  to the distribution $\rho(\x)$. We say that the
  labels in $Q(A,n)$ {\em support} a specialist $B \in S$ if $B
  \subseteq A$.
\item We define the restriction of a specialist set $S$ to a set of
  examples $\K \subset \X$ as:
  $$
  S \restrictedto \K \doteq \left\{ B \in S : | B \cap \K | > 0 \right\}
  $$
\end{itemize}

\begin{algorithm}[]
   \caption{An Active Learning Algorithm}
   \label{alg:activealg}
\begin{algorithmic}
   \STATE \textbf{\textcolor{blue}{Initialize}}   $\K_0=\emptyset$, $\SS_0 = S$,
   \FOR{$i = 1,\ldots$ }
   \STATE \textbf{\textcolor{blue}{Query}}: Generate two new sets of labeled examples, $U_i$ and $F_i$. 
   \STATE{$U_i = Q(\X,n)$}
   \IF{$|\K_{i-1}|>0$}
   \STATE{$F_i = Q(\K_{i-1},n)$ and $\SS_i = \SS_{i-1} \cup (S \restrictedto \K_{i-1})$}
   \akshay{This can be changed when we define a specialist hierarchy; as it stands, it seems to me that $\SS_0 = S$ throughout. }
   \ELSE
   \STATE{$F_i = \emptyset,\; \SS_i=\SS_{i-1}$}
   \ENDIF
   \STATE \textbf{\textcolor{blue}{combine labeled data}}
   \FOR{$B \in \SS_i$}
   \STATE Define the labeled set for $B$ as:
   $$ L_i(B) = \left[ \cup_{j=1}^i U_j \bigcup \cup_{j \leq i: \{B \subseteq \K_j\}} F_j)\right] \restrictedto B$$
   \akshay{I think here $L_i$ should just be the set in brackets, not ``$\restrictedto B$".}
   \ENDFOR
   \STATE \textbf{\textcolor{blue}{Compute Constraints}}
   \FOR{$B \in \SS_i$}
   \STATE{Compute the empirical mean label for $B$:
     $$\emprho(B) = \frac{\sum_{(\x,y) \in L_i(B) }y}{|L_i(B)|}$$}
   \STATE{Compute the error bound for $B$:
     $$\Delta(B) = \sqrt{\frac{\ln 2i^2/\delta + (d+1) \ln
         |L_i(B)|}{|L_i(B)|/2}} $$
   where $d$ is the dimension of the space and $\delta$ is the
   confidence parameter.}
   \STATE Set the confidence interval for $B$ to be 
   of $C_i(B)=[\emprho(B) \pm \Delta(B)] \cap [-1,+1]$ 
   \ENDFOR
   \STATE \textbf{\textcolor{blue}{Solve}}
   \STATE{Solve the linear constraints $\{L_i(B)\}$ and find, for each $\x \in \X$, 
   the segment of feasible solutions $f(\x)$; 
   \akshay{I understand this to be the set of predictions $\subseteq [-1,1]$ on $\x$ that are possible minimax optimal muffled predictions.} 
   Define the new set of known unknowns to be
   $$ \K_i = \{\x | 0 \in f(\x)\} $$}
   \akshay{I believe you mean $f(\x)$ as defined above. In this case, 
   the unsafe set I defined in the earlier writeup (by analogy with CAL-style algorithms; call it $N_i$) is a superset of $\K_i$, 
   because $N_i$ includes any point $\x$ which can be hedged, not just the ones which can have score $0$. }
   \ENDFOR
\end{algorithmic}
\end{algorithm}


The analysis of the algorithm consists of two steps.
First, we analyze an idealized algorithm
which operates under the assumption that exact conditional
probabilities are available. Then, in the second step, we remove the
assumption of exact conditional probabilities, replace it with
estimates based on a sample, and analyze the sample based algorithm.

We define the set of all balls whose support size is at most $k$ by $S_k$.

The parametrized algorithm $I(k)$ uses $S_k$ as the set of specialists. 
It receives as input the exact bias $\bias(B)$ for all $B \in S_r$.

The algorithm solves the min-max weighting of the rules $\sigma^*(r)$.

The two main lemmas we use to analyze this algorithm are:
\begin{lemma}[Subset domination]
  Suppse $B_1$ and $B_2$ are two balls that cover a point $\x$,
  suppose also that $B_2 \subset B_1$ (strict subset), then
  removing $B_1$ from the set of constraints does not change the
  optimal prediction on $\x$.
\end{lemma}

\akshay{I agree that this is good for the analysis. However I am not sure how to prove it. \\ \\
Here is one idea: this will happen if $B_2$ is small enough (suppose e.g. 
that the balls have support size $2^i$ for integer $i$; then each specialist's prediction is  
weighted proportionally to this $2^i$ by the muffler aggregation, 
so if not too many balls of the same size predict at $\x$, 
then the optimal prediction on $\x$ is essentially determined by the smallest specialist predicting on it). 
}

An adaptive version of the VC bound:
\begin{lemma}
Suppose $L$ is a set of labeled examples chosen IID from a
joint distribution over $\X \times Y$. Suppose $S$ is a set of
specialists of VC dimension $d$.
Let $m(B)=|L \cap B|$ denote the number of elements of $L$ that are
inside $B \in S$, let $\rho(B)$ be the mean of the labels in $\X \cap B$ and
let $\emprho(B)$ be the empirical average of the labels in $L \cap B$. 

Then for all $\delta>0$:
$$
Pr\left[ \exists B\in S \text{ s.t. }  |\rho(B) - \emprho(B)| >
  \sqrt{\frac{\ln 2/\delta+d \ln m(B)}{m(B)/2}} \right] < \delta
$$
\end{lemma}




\end{document}
