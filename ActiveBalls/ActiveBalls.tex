\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}

\usepackage{hyperref}
\usepackage{url}
\usepackage{times}
\usepackage[algo2e]{algorithm2e}

%\usepackage{fullpage}
%\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{bbm}
\usepackage{graphics, graphicx, xcolor}
\usepackage{enumitem}
%\usepackage{verbatim}		% for misc commenting, etc.
\usepackage{stmaryrd}
\usepackage{float}
\usepackage[mathscr]{euscript}


\usepackage{geometry}
%% \geometry{a4paper,
%%   total={170mm,220mm},
%%   marginparwidth=80mm,
%% left=5mm,
%% right=85mm,
%% top=20mm,
%% }

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}


\title{Active learning using muffler}
\author{Akshay Balsubramani, Yoav Freund, Shay Moran}
\date{November 2016}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}[theorem]{assumption}
\newtheorem{definition}[theorem]{Definition}

\newtheorem{thm}{Theorem}%[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{obs}[thm]{Observation}
\newtheorem{defn}[thm]{Definition}
\newtheorem{alg}{Algorithm}
\newtheorem{ass}{Assumption}
\newtheorem{examp}{Example}
\newtheorem{property}{Property}
\setcounter{MaxMatrixCols}{20}

\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Prtxt}{Pr}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\polylog}{polylog}

\newcommand{\err}{\mbox{err}}
\newcommand{\X}{{\cal X}}
\newcommand{\Y}{{\cal Y}}
\newcommand{\D}{{\cal D}}
\newcommand{\B}{{\cal B}}
\newcommand{\x}{\vec{x}}
\newcommand{\y}{\vec{y}}
\newcommand{\vv}{\vec{v}}
\newcommand{\cc}{\vec{c}}

\newcommand{\K}{{\cal K}}
\newcommand{\restrictedto}{\triangleright}
\renewcommand{\SS}{{\cal S}} % Specialists
\newcommand{\CC}{{\cal C}}  % constraints

\newcommand{\outcome}{z}
\newcommand{\empoutcome}{\hat{\outcome}}
\newcommand{\polarity}{p}

\newcommand{\bd}[1]{\mathbf{#1}}  % for bolding symbols
\newcommand{\RR}{\mathbb{R}}      % Real numbers
\newcommand{\ZZ}{\mathbb{Z}}      % Integers
\newcommand{\NN}{\mathbb{N}}      % natural numbers
\newcommand{\RP}{\mathbb{RP}}      % real projective space
\newcommand{\Sp}{\mathbb{S}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\col}[1]{\left[\begin{matrix} #1 \end{matrix} \right]}
\newcommand{\comb}[2]{\binom{#1^2 + #2^2}{#1+#2}}
\newcommand{\vnorm}[1]{\left\lVert#1\right\rVert} % vector norm
\newcommand{\bfloor}[1]{\left\lfloor#1\right\rfloor} % floor function
\newcommand{\bceil}[1]{\left\lceil#1\right\rceil} % ceiling function
\newcommand{\ifn}{\mathbf{1}} % indicator function for sets
\newcommand{\EV}{\mathbb{E}} % expected value operator
\newcommand{\evp}[2]{\mathbb{E}_{#2} \left[#1\right]} % expected value operator
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\pr}[1]{\Prtxt \left(#1\right)}
\newcommand{\prp}[2]{\Prtxt_{#2} \left(#1\right)}
\newcommand{\ip}[2]{\left\langle #1, #2 \right\rangle}
\newcommand{\emperr}[2]{\widehat{\mbox{err}}_{#2} \left(#1\right)}

\newcommand{\pdis}[1]{P_{dis}\left(#1\right)}
\newcommand{\lrp}[1]{\left(#1\right)}
\newcommand{\lrb}[1]{\left[#1\right]}
\newcommand{\lrsetb}[1]{\left\{#1\right\}}

\newcommand{\corr}{\mbox{corr}}
\newcommand{\ones}[1]{\mathbbm{1}^{#1}}
\newcommand{\vA}{\mathbf{A}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vd}{\mathbf{d}} 
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vF}{\mathbf{F}} 
\newcommand{\vI}{\mathbf{I}}  
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vb}{\mathbf{b}} 
\newcommand{\vu}{\mathbf{u}}   
\newcommand{\vl}{\mathbf{l}}
\newcommand{\vm}{\mathbf{m}}    
\newcommand{\vg}{\mathbf{g}}   
\newcommand{\vp}{\mathbf{p}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vs}{\mathbf{s}}
\newcommand{\vt}{\mathbf{t}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\valpha}{\vec{\alpha}}
\newcommand{\vbeta}{\vec{\beta}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\vone}{\mathbf{1}}

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cO}{\mathcal{O}}

\newcommand{\bias}{\text{bias}}
\newcommand{\ebias}{\widehat{\text{bias}}}

\newcommand{\eD}{\hat{\D}}
\newcommand{\ep}{\hat{p}}

\newcommand{\sign}{\text{sign}}
\newcommand{\new}[1]{\textcolor{red}{#1}}

\newcommand{\comment}[3]{\marginpar{\textcolor{#2}{#1: #3}}}
%\newcommand{\comment}[3]{}
\newcommand{\shay}[1]{\comment{Shay}{red}{#1}}
\newcommand{\yoav}[1]{\comment{Yoav}{blue}{#1}}
\newcommand{\akshay}[1]{\comment{Akshay}{magenta}{#1}}

\begin{document}

\maketitle

\section{Introduction}

As a first step towards using Muffler for active learning, we describe
a setup in which Muffler converges to the Bayes optimal rule.

We operate in a restricted context which emulates the kNN 
convergence rate analysis of Chaudhuri and Dasgupta.

\section{Preliminaries}
\label{sec:setup}

The main tools we use in this paper are linear programming and uniform
convergence. We therefore use a combination of matrix notation and
the probabilistic notation given in the introduction. 
The algorithm is first described in a deterministic context where some inequalities are assumed to hold; probabilistic
arguments are used to show that these assumptions are correct with high probability.

The ensemble's predictions on the unlabeled data are denoted by $\vF$:
\begin{equation}
\vF = 
 \begin{pmatrix}
   h_1(x_1) & h_1(x_2) & \cdots & h_1 (x_n) \\
   \vdots   & \vdots    & \ddots &  \vdots  \\
   h_p(x_1)  &  h_p (x_2)  & \cdots &  h_p (x_n)
 \end{pmatrix}
 \in [-1, 1]^{p \times n}
\end{equation}
The \textbf{true labels} on the test data $U$ are represented by $\vz
= (z_1; \dots; z_n) \in [-1,1]^n$.

Note that we allow $\vF$ and $\vz$ to
take any value in the range $[-1, 1]$ rather than just the two
endpoints. This relaxation does not change the analysis, because intermediate
values can be interpreted as the expected value of randomized
predictions.  For example, a value of $\frac{1}{2}$ indicates $\{+1\;
\text{w.p.}\; \frac{3}{4} $, $-1\; \text{w.p.}\; \frac{1}{4} \}$. This
interpretation extends to our definition of the correlation
on the test set,
$\widehat{\corr}_{U} (h_i) = \frac{1}{n} \sum_{j=1}^n h_i (x_j) z_j$.~\footnote{We are slightly abusing the
  term ``correlation'' here. Strictly speaking this is just the expected
  value of the product, without standardizing by mean-centering and rescaling for unit variance. 
  We prefer this to inventing a new term.}

The labels $\vz$ are hidden from the predictor, 
but we assume the predictor has knowledge of a {\bf correlation vector}
$\vb \geq \vzero^n$ such that $\widehat{\corr}_{U} (h_i) \geq b_i$ for all $i \in [p]$, 
i.e. $ \frac{1}{n} \vF \vz \geq \vb$. 
From our development so far, the correlation vector's components $b_i$ each correspond 
to a constraint on the corresponding classifier's test error $\frac{1}{2} (1 - b_i)$. 

The following notation is used throughout the paper: $[a]_{+} = \max (0, a)$ and $[a]_{-} = [-a]_{+}$,  
$[n] = \{ 1,2,\dots,n \}$, $\vone^n = (1; 1; \dots; 1) \in \RR^n$, and $\vzero^n$
similarly.  Also, write $I_n$ as the $n \times n$ identity matrix.
All vector inequalities are componentwise. 
The probability simplex in $d$ dimensions is denoted by $\Delta^d = \{ \sigma \geq \vzero^d : \sum_{i=1}^d \sigma_i = 1 \}$.
Finally, we use vector notation for the rows and columns of $\vF$: 
$\vh_i = (h_i(x_1), h_i(x_2), \cdots, h_i (x_n))^\top$ and $\vx_j =
(h_1(x_j), h_2(x_j), \cdots, h_p (x_j))^\top$.

%----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\section{The Transductive Binary Classification Game}
\label{sec:game1}

We now describe our prediction problem, and formulate it as a zero-sum game between 
two players: a predictor and an adversary.

In this game, the predictor is the first player, 
who plays $\vg = (g_1; g_2; \dots; g_n)$, 
a randomized label $g_i \in [-1,1]$ for each example $\{\vx_i\}_{i=1}^{n}$. 
The adversary then plays, setting the labels $\vz \in [-1,1]^n$ 
under ensemble test error constraints defined by $\vb$. 
The predictor's goal is to minimize (and the adversary's to maximize) 
the \emph{worst-case expected classification error on the test data} 
(w.r.t. the randomized labelings $\vz$ and $\vg$): 
$\frac{1}{2} \lrp{1 - \frac{1}{n} \vz^\top \vg }$. 
This is equivalently viewed as maximizing worst-case correlation $\frac{1}{n} \vz^\top \vg $. 

To summarize concretely, we study the following game:
\begin{align}
\label{game1eq}
\displaystyle 
V := \max_{\vg \in [-1,1]^n} \; \min_{\substack{ \vz \in [-1,1]^n ,
    \\ \frac{1}{n} \vF \vz \in [\vb_l,\vb_u] }} \;\; \frac{1}{n} \vz^\top \vg
\end{align}

It is important to note that we are only modeling ``test-time''
prediction, and represent the information gleaned from the labeled data
by the parameter $\vb$. Inferring the vector $\vb$ from training data
is a standard application of Occam's Razor \cite{BEHW87}, which we provide in
Section~\ref{sec:uniform-convergence}.

The minimax theorem (e.g. \cite{CBL06}, Theorem 7.1) applies to the game \eqref{game1eq}, 
since the constraint sets are convex and compact and the payoff linear. 
Therefore, it has a minimax equilibrium and associated optimal 
strategies $\vg^*, \vz^*$ for the two sides of the game, i.e. 
$\min_{\vz}\; \frac{1}{n} \vz^\top \vg^* = V = \max_{\vg} \frac{1}{n} \vz^{*^\top} \vg$ .
%\begin{align*}
%\min_{\substack{ \vz \in [-1,1]^n , \\ \frac{1}{n} \vF \vz \geq \vb }}\; \frac{1}{n} \vz^\top \vg^* 
%= V = \max_{\vg \in [-1,1]^n} \frac{1}{n} \vz^{*^\top} \vg
%\end{align*}

As we will show, both optimal strategies are simple functions of a
particular \emph{weighting} over the $p$ hypotheses -- a nonnegative $p$-vector. 
Define this weighting as follows.
\begin{defn}[\textbf{Slack Function and Optimal Weighting}]
Let $\sigma \geq 0^p$ be a weight vector over $\cH$ (not necessarily a distribution).
The vector of \textbf{ensemble predictions} is
$\vF^\top \sigma = (\vx_1^\top \sigma, \dots, \vx_n^\top \sigma)$, 
whose elements' magnitudes are the \textbf{margins}. 
The \textbf{prediction slack function} is
\begin{align}
\label{eqn:slack}
\gamma (\sigma, \vb) = \gamma (\sigma) := \frac{1}{n} \sum_{j=1}^n \left[ \abs{\vx_{j}^\top \sigma} - 1 \right]_{+} - \vb^\top \sigma
\end{align}
An \textbf{optimal weight vector} $\sigma^*$ is any minimizer of the slack function: 
$\displaystyle \sigma^* \in \argmin_{\sigma \geq 0^p} \left[ \gamma (\sigma) \right]$.
\end{defn}

Our main result uses these to describe the solution of the game \eqref{game1eq}.
\begin{thm}[Minimax Equilibrium of the Game]
\label{thm:gamesolngen}
The minimax value of the game \eqref{game1eq} is 
$V = - \gamma (\sigma^*)$. 
The minimax optimal strategies are defined as follows:
for all $i \in [n]$,
\begin{align}
g_i^* \doteq g_i (\sigma^*) = \begin{cases} \vx_{i}^\top \sigma^* & \abs{\vx_{i}^\top \sigma^*} < 1 \\ 
\sgn(\vx_{i}^\top \sigma^*) & \mbox{otherwise} \end{cases}
\quad \quad \text{and} \quad \quad
z_i^* = 
\begin{cases} 
0 & \abs{\vx_{i}^\top \sigma^*} < 1 \\ 
\sgn(\vx_{i}^\top \sigma^*) & \abs{\vx_{i}^\top \sigma^*} > 1 
\end{cases}
\label{eqn:opt-strats}
\end{align}
\end{thm}

\begin{figure}
\centering
\includegraphics[width=0.55\textwidth]{figs/optstrats.png}
\caption{
\label{fig:optstrats}
The optimal strategies and slack function as a function of the ensemble prediction $\vx^\top \sigma^*$.}
\end{figure}

The proof of this theorem is a standard application of Lagrange duality and the minimax theorem.
The minimax value of the game and the optimal strategy for the predictor $\vg^*$ (Lemma~\ref{lem:game1gopt}) 
are our main objects of study and are completely characterized, 
and the theorem's partial description of $\vz^*$ (proved in Lemma \ref{lem:game1zopt}) 
will suffice for our purposes. 
\footnote{For completeness, Corollary \ref{cor:zoptfullpred} in the appendices 
specifies $z_i^*$ when $\abs{\vx_{i}^\top \sigma^*} = 1$.}

Theorem \ref{thm:gamesolngen} illuminates the importance of the optimal weighting $\sigma^*$ over hypotheses. 
This weighting $\sigma^* \in \argmin_{\sigma \geq 0^p} \gamma (\sigma)$ is the solution 
to a convex optimization problem (Lemma \ref{lem:helperthreshcvx}), 
and therefore we can efficiently compute it and $\vg^*$ to any desired accuracy. 
The ensemble prediction (w.r.t. this weighting) on the test set is $\vF^\top \sigma^*$, 
which is the only dependence of the solution on $\vF$. 

More specifically, the minimax optimal prediction and label \eqref{eqn:opt-strats} on any test set example $\vx_j$ 
can be expressed as functions of the ensemble prediction $\vx_j^\top \sigma^*$ 
on that test point alone, without considering the others. 
The $\vF$-dependent part of the slack function also depends separately on each test point's ensemble prediction. 
Figure \ref{fig:optstrats} depicts these three functions. 





\section{Ball Specialists}

We restrict our attention to a special case which corresponds,
roughly, to nearest neighbor methods.
\begin{enumerate}
\item The input space $\X$ is a finite set in $R^d$. We assume a
  uniform distribution over $\X$.~\footnote{We use an arrow notation
    $\x$ to differentiate between $\x \in R^d$ and $\vx$ which denotes
    a row of the matrix $\vF$.}
  \item
    The rules that we use are ``specialists'' that are balls. The set
    $\B$ contains all rules of the form
    \[
    B_{r,\cc,s}(\x) =
    \begin{cases}
      s & \text{if } \| \cc- \x \| \leq r \\
    0 & \text{otherwise }
    \end{cases}
    \]
    Where $r \geq 0$ is the radius of the ball, $\cc \in R^d$ is the
    center of the ball and $s \in \{-1,+1\}$ is the polarity of the ball.
    We will drop the subscripts of $B$ when clear from context.
  \item
    We use $\x \in B$ to indicate that $B(\x) \neq 0$.
  \item
    We denote the {\em probability} of a ball $B$ by $p(B) \doteq
    \frac{|B|}{|\X|}$
  \item
    We use the term {\em bias} of a ball to refer to the conditional
    expectation of the label for a ball by
    $$
    \bias(B) \doteq E\left( y|\x \in B \right)
    $$
\end{enumerate}

\newcommand{\edge}{\gamma}
\newcommand{\SepsGamSig}{\SS_{\epsilon,\edge}^{s}}
\newcommand{\SepsGam}{\SS_{\epsilon,\edge}}
\newcommand{\SepsGamMinusSig}{\SS_{\epsilon,\edge}^{-s}}
\newcommand{\ConstrepsGamSig}{\CC_{\epsilon,\edge}^{s}}
\newcommand{\ConstrepsGam}{\CC_{\epsilon,\edge}}
\newcommand{\ConstrepsGamMinusSig}{\CC_{\epsilon,\edge}^{-s}}

\section{Degrees of Safety}
We say that a point $\x \in \X$ is {\em safe} if we can confidently identify
the label of $\x$. We distinguish three levels of safety of
increasing strength: version-space (VS) safety, pairwise (PW) safety and
asymptotic (A) safety. We define each one in turn.

First, we need some notation. We denote the set of all balls by $\SS$. For any $\epsilon,\edge>0$
and $s \in \{-1,+1\}$ we define the set of $(\epsilon,\edge,s)$-
good balls $\SepsGamSig \subset \SS$ to be:
\[
\SepsGamSig \doteq \left\{B \in \SS \left| p(B) \geq \epsilon,
s\; \bias(B) > \edge \right. \right\}
\]
We define $\SepsGam^{\pm} \doteq \SepsGam^{+} \cup \SepsGam^{-}$

Denote by $V(\SepsGam^{\pm})$ the set of all point-wise biases $\vz$
that satisfy the constraints defined by $\SepsGam^{\pm}$

\newcommand{\XepsGamSig}{\X_{\epsilon,\edge}^{s}}
\newcommand{\XepsGam}{\X_{\epsilon,\edge}}
\newcommand{\XpesGamPol}{\X_{\epsilon,\edge}^{\mbox{pol}}}

\begin{itemize}
\item {\bf Version space (VS) safety}\\
We are $(\epsilon,\edge,s)$-{\bf VS safe} on $\x$ if $s\cdot\sign(\vz^*(\x)) \geq 0$
for $\vz^*$ that satisfy  $\frac{1}{n} \vF \vz^* \geq \vb$ and are
min/max optimal.

\item
{\bf Pair-Wise (PW) safety}\\
We define the set of $(\epsilon,\edge,s)$-{\bf PW safe} instances
to be 
\[
\XepsGamSig \doteq \left\{\x \in R^d \left|
\begin{array}{l}
\exists B \in \SepsGamSig \mbox{ s.t. } \x \in B \mbox{ and } \\
\forall B \in \SepsGamMinusSig \mbox{ s.t. } \x \in B,\;\;
\exists B' \in \SepsGamSig \mbox{ s.t. } \x \in B' \mbox{ and } B'
\subset B
\end{array}
\right. \right\}
\]

\item
{\bf Asymptotic (A) Safety}\\ We say that $\x$ is $(\epsilon,\edge,s)$- {\bf A-safe} if
it is $(\epsilon,\gamma',s)$-{\bf PW safe} for all $0<\epsilon' \leq
\epsilon$ and $0<\gamma' \leq \gamma$ and for the same polarity $s$.
\end{itemize}


\section{Pairwise safety implies version space safety}

\newcommand{\AllSet}{{\cal A}(\x,\epsilon,\edge)}
\newcommand{\MinSet}{{\cal M}(\x,\epsilon,\edge)}

Fix a point $\x$ and the parameters $(\epsilon,edge,s)$.
Let $\AllSet$ be the sets of all balls $B$ that contain $\x$, have weight $\epsilon>0$
and edge $\gamma$ with respect to {\em some} polarity $s \in
\{-1,+1\}$. In other words:
\[
\AllSet \doteq \left\{ B \left|
\frac{|B|}{|\X|} \geq \epsilon\;\;\mbox{ and }\;\;
\exists s \in \{-1,+1\}\;\; \mbox{ such that }\;\; \frac{s}{|B|}
\sum_{\x \in B} \vz(\x) \geq \edge \right. \right\}
\]

Consider the partial order of containment defined over the balls in
$\AllSet$.
Let the ``set of minima'' $\MinSet \subseteq \AllSet$ be the set of balls that are
minimial with respect to this partial order. An alternative
specification of pairwise safety is that that all balls in $\MinSet$
set have the same polarity $s$. More formally, $\x$ is
$(\epsilon,\edge,s)$-pairwise safe if and only if
\[
\forall B \in \MinSet,\;\;\; \frac{s}{|B|} \sum_{\x \in B} \vz(\x) \geq \edge 
 \]

Before proving that Pairwise Safety implies Version Space safety, we
need the following technical lemma:


\begin{lemma}
  Let $\cA=\{A_1,A_2,\ldots,A_n\}$ be a finite collection of non-empty sets over a finite
  domain. Then there exist a set of at most $n$ points
  $x_1,\ldots,x_m$ such that each set in $\cA$, contains exactly one point.
\end{lemma}
\begin{proof}
  Denote by $\cC$ a collection of sets. We use the notation $\cap \cC$
  to denote the intersection of the sets in $\cC$.

  The proof is constructive and recursive. We start with $\cC=\cA$ and
  continue until $\cC$ is empty.
  
  At each stage of the recursion we distinguish two cases:
  \begin{enumerate}
    \item $\cap \cC \neq \emptyset$. In this case
      We choose $\x$ from the intersection of all sets and we are
      done.
    \item $\cap \cC = \emptyset$. In this case we partition $\cC$ into
      two disjoint collections $\cD$ and $\cF$, such that $\cap \cD
      \neq \emptyset$ and for all $A \in \cF$, $\cap \cD \cap A =
      \emptyset$. Note that $x \in A$ for all $A \in \cD$ and $x \notin A$
      for $A \in \cF$. We choose an arbitrary $x$ element from $\cap
      \cD$ and we can remove $\cD$ from consideration
      and continue recursively with $\cC = \cF$.

      The construction of the collection $\cD$ is greedy. We start
      with an arbitrary set $A_1$ in $\cC$, by assumption, this set is
      not empty. We then repeatedly add sets to $\cD$ as long as their
      addition does not result in $\cap \cD \neq \emptyset$. As $\cap
      \cC=\emptyset$ we know that this process must at some point before
      $\cD=\cC$. We define $\cD$ to be a collection of sets whose
      intersection is not empty such that the addition of any set will
      make the intersection empty.

      In other words, any point $x$ chosen from $\cap \cD$ is a member
      of $A \in \cD$ and is not a member of $ \in \cF$
    \end{enumerate}
\end{proof}

We will not prove the theorem:
\begin{theorem}
  If $\vx$ is pairwise safe for some $\epsilon,\edge,s$ then it is version space safe. 
\end{theorem}

x
\begin{proof}

The proof is by contradiction.

  
Assume that $\vx$ is $(\epsilon,\edge,s)$-PW safe but that it is not
$(\epsilon,\edge,s)$-VS safe. In other words, there exists
a min-max optimal adversarial strategy $\vz^*$ which satisfies all of the
pair-wise constraints regarding $\vx$ but for which $s \cdot
\vz^*(\vx)<0$.

We will show that, under these conditions, there is another
adversarial strategy $\vz'$ which guarantees a better value for the
adversary and satisfies all of the constraints.

\end{proof}


\end{document}
