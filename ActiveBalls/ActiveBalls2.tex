\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}

\usepackage{hyperref}
\usepackage{url}
\usepackage{times}
\usepackage[algo2e]{algorithm2e}

%\usepackage{fullpage}
%\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{bbm}
\usepackage{graphics, graphicx, xcolor}
\usepackage{enumitem}
%\usepackage{verbatim}		% for misc commenting, etc.
\usepackage{stmaryrd}
\usepackage{float}
\usepackage[mathscr]{euscript}


\usepackage{geometry}
%% \geometry{a4paper,
%%   total={170mm,220mm},
%%   marginparwidth=80mm,
%% left=5mm,
%% right=85mm,
%% top=20mm,
%% }

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}


\title{Non-Parametric Active Learning}
\author{Akshay Balsubramani, Yoav Freund, Shay Moran}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}[theorem]{assumption}
\newtheorem{definition}[theorem]{Definition}

\newtheorem{thm}{Theorem}%[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{obs}[thm]{Observation}
\newtheorem{defn}[thm]{Definition}
\newtheorem{alg}{Algorithm}
\newtheorem{ass}{Assumption}
\newtheorem{examp}{Example}
\newtheorem{property}{Property}
\setcounter{MaxMatrixCols}{20}

\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Prtxt}{Pr}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\polylog}{polylog}

\newcommand{\err}{\mbox{err}}
\newcommand{\X}{{\cal X}}
\newcommand{\Y}{{\cal Y}}
\newcommand{\D}{{\cal D}}
\newcommand{\B}{{\cal B}}
\newcommand{\x}{\vec{x}}
\newcommand{\y}{\vec{y}}
\newcommand{\vv}{\vec{v}}
\newcommand{\cc}{\vec{c}}

\newcommand{\K}{{\cal K}}
\newcommand{\restrictedto}{\triangleright}
\renewcommand{\SS}{{\cal S}} % Specialists
\newcommand{\CC}{{\cal C}}  % constraints

\newcommand{\outcome}{z}
\newcommand{\empoutcome}{\hat{\outcome}}
\newcommand{\polarity}{p}

\newcommand{\bd}[1]{\mathbf{#1}}  % for bolding symbols
\newcommand{\RR}{\mathbb{R}}      % Real numbers
\newcommand{\ZZ}{\mathbb{Z}}      % Integers
\newcommand{\NN}{\mathbb{N}}      % natural numbers
\newcommand{\RP}{\mathbb{RP}}      % real projective space
\newcommand{\Sp}{\mathbb{S}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\col}[1]{\left[\begin{matrix} #1 \end{matrix} \right]}
\newcommand{\comb}[2]{\binom{#1^2 + #2^2}{#1+#2}}
\newcommand{\vnorm}[1]{\left\lVert#1\right\rVert} % vector norm
\newcommand{\bfloor}[1]{\left\lfloor#1\right\rfloor} % floor function
\newcommand{\bceil}[1]{\left\lceil#1\right\rceil} % ceiling function
\newcommand{\ifn}{\mathbf{1}} % indicator function for sets
\newcommand{\EV}{\mathbb{E}} % expected value operator
\newcommand{\evp}[2]{\mathbb{E}_{#2} \left[#1\right]} % expected value operator
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\pr}[1]{\Prtxt \left(#1\right)}
\newcommand{\prp}[2]{\Prtxt_{#2} \left(#1\right)}
\newcommand{\ip}[2]{\left\langle #1, #2 \right\rangle}
\newcommand{\emperr}[2]{\widehat{\mbox{err}}_{#2} \left(#1\right)}
\newcommand{\Ex}{\mathbb{E}} % expected value operator


\newcommand{\pdis}[1]{P_{dis}\left(#1\right)}
\newcommand{\lrp}[1]{\left(#1\right)}
\newcommand{\lrb}[1]{\left[#1\right]}
\newcommand{\lrsetb}[1]{\left\{#1\right\}}

\newcommand{\corr}{\mbox{corr}}
\newcommand{\ones}[1]{\mathbbm{1}^{#1}}
\newcommand{\vA}{\mathbf{A}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vd}{\mathbf{d}} 
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vF}{\mathbf{F}} 
\newcommand{\vI}{\mathbf{I}}  
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vb}{\mathbf{b}} 
\newcommand{\vu}{\mathbf{u}}   
\newcommand{\vl}{\mathbf{l}}
\newcommand{\vm}{\mathbf{m}}    
\newcommand{\vg}{\mathbf{g}}   
\newcommand{\vp}{\mathbf{p}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vs}{\mathbf{s}}
\newcommand{\vt}{\mathbf{t}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\valpha}{\vec{\alpha}}
\newcommand{\vbeta}{\vec{\beta}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\vone}{\mathbf{1}}

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cO}{\mathcal{O}}

\newcommand{\bias}{\text{bias}}
\newcommand{\ebias}{\widehat{\text{bias}}}

\newcommand{\eD}{\hat{\D}}
\newcommand{\ep}{\hat{p}}

\newcommand{\samp}{S}
\newcommand{\usamp}{\underline{S}}

\newcommand{\eps}{\epsilon}

\newcommand{\sign}{\text{sign}}
\newcommand{\new}[1]{\textcolor{red}{#1}}

\newcommand{\comment}[3]{\marginpar{\textcolor{#2}{#1: #3}}}
%\newcommand{\comment}[3]{}
\newcommand{\shay}[1]{\comment{Shay}{red}{#1}}
\newcommand{\yoav}[1]{\comment{Yoav}{blue}{#1}}
\newcommand{\akshay}[1]{\comment{Akshay}{magenta}{#1}}

\begin{document}

\maketitle
\section{Introduction}

The analysis of Active Learning using Muffler seems to run into a dead
end. Specifically, the elegant local condition we call ``Pairwise
safe'' is insufficient to guarantee version space consistency.

In this paper we suggest a different approach to the problem of active
learning using balls. Instead of using the min/max solution given by
Muffler, we consider an Occam's razor approach by which we make the
simplest prediction possible given the constraints.

\section{Ball Specialists}

We restrict our attention to a special case which corresponds,
roughly, to nearest neighbor methods.
\begin{enumerate}
\item The input space $\X$ is a finite set in $R^d$. We assume a
  uniform distribution over $\X$.
  \item We consider labels $y\in \{-1,+1\}$. For There is a fixed but
    unknown conditional probability defined over $\X$, i.e. $P(Y=+1 |
    X=\x)$. Our goal is to estimate whether $P(Y=+1|X=\x)$ is smaller
    or larger than $1/2$.
  \item
    The notation is simpler if we use expected values instead of
    probabilities. We use the term {\em bias} of a ball to refer to the conditional
    expectation of the label for a ball by
    \[
    \bias(\x) \doteq P(Y=+1|X=\x) - P(Y=-1|X=\x)
    \]
  \item
    The rules that we use are ``specialists'' that are balls. The set
    $\B$ contains all rules of the form
    \[
    B_{r,\cc,s}(\x) =
    \begin{cases}
      s & \text{if } \| \cc- \x \| \leq r \\
    0 & \text{otherwise }
    \end{cases}
    \]
    Where $r \geq 0$ is the radius of the ball, $\cc \in R^d$ is the
    center of the ball and $s \in \{-1,+1\}$ is the polarity of the ball.
    We will drop the subscripts of $B$ when clear from context.
  \item
    We use $\x \in B$ to indicate that $B(\x) \neq 0$.
  \item
    We denote the {\em probability} of a ball $B$ by $p(B) \doteq
    \frac{|B|}{|\X|}$.
  \item
    We define term {\em bias} of a ball to refer to the conditional
    expectation of the label for a ball by
    \[
    \bias(B) \doteq E\left( y|\x \in B \right).
    \]
  \item We define a sample $\samp$ as a sequence of labeled examples:
    \[\samp= \left\langle (\x_1,y_1),(\x_2,y_2),\ldots,(\x_m,y_m)
    \right\rangle \]
    Where $\x_i$ are chosen uniformly at random (with replacement)
    from $\X$ and $y_i$ are chosen according to the (unknown)
    conditional distribution of the label given $\x$.
  \item Given a sample $\samp$, we define the number of instances in
    $(\x,y) \in \samp$ such that $\x \in B$ to be the {\em size} of
    $B$ according to the sample $\samp$ and denote it by $k_{\samp}$. We define
    the {\em empirical probability} of the ball $B$ according to
    $\samp$ by $p_{\samp}(B) \doteq k_{\samp}(B)/|\samp|$.
  \item Given a sample $\samp$ we define the estimate of the
    $\bias(B)$ to be
    \[
    \ebias_{\samp}(B) = \frac{\sum_{i=1}^m y_i B(\x_i)}{\sum_{i=1}^m B(\x_i)}
    \]
  \item Using uniform convergence bounds we define for each ball $B$
    a confidence interval:
    $[l,h]=[\ebias(B)-\Delta,\ebias(B)+\Delta]$.
    if $l>0$ we say that $B$ imposes a {\em positive constraint}, if
    $h<0$ we say that $B$ imposes a {\em negative constraint}, i
    $l\leq 0 \leq h$ we say that $B$ does not impose a constraint.
\end{enumerate}

\section{Uniform convergence bounds for sepcialists.}
%\subsection{Uniform convergence of measures}
%
%In order for our unifom convergence bounds to hold, we need to remove
%from consideration all balls in which the number of points is smaller
%than some minimum $O(d \log n + \ln 1/\delta)$. As this threshold is
%used in the algorithm we need to be specific and we choose
%$k_0=50\bigl(d\log(2n) + \log(10/\delta)\bigr)$ to be the threshold.
%
%It might be the case that by properly chosing the length of the
%confidence interval we can make sure that when $k(ball)<2k_0$ the
%inteval would make the statement vacuous. Restricting $k(B)>2k_0$
%gives a more direct proof. The bound is still better than the uniform
%bound that depends on $n$ rather than on $k(B)$.
%
%The first thing we show is that when using a sample of size $2n$, the
%probability that there exists a ball $B$ such that $k_1(B)>2k_0$ and
%$k_2(b)<k_0/2$ is smaller than $\delta$. In other words, the
%probability that at least one of the balls used by the algorithm has
%$k_2(B)<k_0/2$ is at most $\delta$.
%
%%\begin{theorem}\label{thm:uc1}
%%Let $\B$ be a family of specialists of VC dimension $d$,
%%let $\delta>0$ and set $k_0=3\left[  d \log n + \log(1/\delta) \right]$
%%and let $S_1,S_2$ be two independent samples $\bigl((x_i,y_i)\bigr) \sim p^n$. Then:
%%\[\Pr_{\samp_1,\samp_2 \sim p^{2n}}\Bigl[\exists {B\in\B}:
%%  k_{\samp_1}(B)>2k_0 \mbox{ and } k_{\samp_2}(B) < \frac{k_0}{2} \Bigr] \leq \delta
%%\]
%%\end{theorem}
%
%\begin{theorem}\label{thm:uc1}
%Let $\B$ be a family of specialists of VC dimension $d$,
%let $\delta>0$, let $k \geq k_0(n,\delta) := \new{50\bigl(d\log(2n) + \log(10/\delta)\bigr)}$,
%and let $S$ be an independent sample $\bigl((x_i,y_i)\bigr) \sim p^n$. Then:
%\[\Pr_{\samp \sim p^{n}}\Bigl[\exists {B\in\B}:
%  p_{\samp}(B)>2k/n \mbox{ and } p_{\samp}(B)\notin [{2^{-1}p(B)}, 2p(B)]  \Bigr] \leq \delta,
%\]
%\end{theorem}
%We will use $k_0=k_0(n,\delta)$ as a threshold for the algorithm:
%balls that contain less than $2k$ points are ignored by it.
%
%\begin{proof}
%We use the standard uniform convergence bound with
%the difference that we use the multiplicative Chernoff bound instead
%of the additive bound. 
%
%We begin with the first inequality.  
%We follow the double sampling argument.
%Consider a double sample $S=(\samp_1,\samp_2)\sim p^{2n}$.
%Let $E_1$ denote the event whose probability we want to bound:
%\[
%E_1 = \text{"}\exists {B\in\B}:
%  p_{\samp_1}(B)>2k/n \mbox{ and } p_{\samp_1}(B)\notin [{2^{-1}p(B)}, 2p(B)]
%\text{"}, 
%\]
%and let $E_2$ denote the event:
%\[
%E_2 = \text{"}\exists {B\in\B}:
%  p_{\samp_1}(B)>2k/n \mbox{ and } p_{\samp_1}(B)\notin \Bigl[\frac{3}{4}p_{\samp_2}(B), \frac{4}{3}p_{\samp_2}(B)\Bigr]
%\text{"}.
%\]
%The proof follows from the following two lemmas:
%\begin{lemma}\label{lem:auxuc1}
%\[\Pr[E_1]\leq 10\Pr[E_2].\]
%\end{lemma}
%\begin{lemma}\label{lem:auxuc2}
%\[\Pr[E_2]\leq \delta/10.\]
%\end{lemma}
%
%\begin{proof}[Proof of Lemma~\ref{lem:auxuc1}]
%
%%\bf{High level.}
%%Condition on $E_1$ and let $B$ be a specialist that certifies it.
%%Since $p(B)$ is small, it follows that with a constant probability
%%$p_{\samp_2}(B)$ is also small.
%%This should imply $E_2$.
%%So, it follows that conditioned on $E_1$, 
%%the event $E_2$ occurs with a constant probability.
%
%It suffices to show that $\Pr[E_2 \vert E_1]\geq 1/10$.
%Indeed, this would imply that $\Pr[E_2 \vert E_1]\geq 1/10$, 
%namely that $\Pr[E_1] \leq 10\Pr[E_1 \land E_2]\leq 10\Pr[E_2]$.
%
%Let $S_1\in E_1$. Since $S_2$ and $S_1$ are independent,
%it suffices to show that 
%\[\Pr_{S_2\sim p^n}\bigl[(S_1,S_2)\in E_2\bigr] \geq 1/10.\]
%Let $B\in\B$ such that $p_{\samp_1}(B)>2k/n \mbox{ and } p_{\samp_1}(B)\notin [{2^{-1}p(B)}, 2p(B)]$.
%We want to show that $p_{\samp_1}(B)\notin\Bigl[\frac{3}{4}p_{\samp_2}(B), \frac{4}{3}p_{\samp_2}(B)\Bigr]$ with probability at least $1/10$.
%It is enough to show that 
%\[ \Pr\Bigl[ p_{\samp_2}(B) \notin[2p(B)/3,3p(B)/2]\Bigr] < 9/10. \]
%
%\new{I am having some problems with this:
%Imagine the case where $p(B)$ is very small, say less than $1/100n$.
%In such a case it will almost surely be the case that $p_{\samp_2}(B)=0$
%which is $\lnot E_2$.}
%
%%\begin{align*}
%%p_2&\in [2p/3,3p/2]]\\
%%p_1&\notin [p/2,2p]\\
%%&\implies\\
%%\text{if }p_1 > 2p &\implies p_1 >  (4/3)(3p/2) \geq (4/3)p_2,\\
%%\text{else, } p_1 < p/2 &\implies p_1 < (3/4)(2p/3) \leq (3/4)p_2. 
%%\end{align*}
%
%Having $p_S(B)\notin[2^{-1}p(B),2p(B]$ means
%that either $p(B) > 2p_S(B)$ or $p(B) < p_S(B)/2$.
%We show that in either way $p_{\samp_2}(B)\notin[3p(B)/4,4p(B)/3]$.
%
%If $p(B) > 2p_{\samp_1}(B)$
%then by the multiplicative Chernoff bound (see e.g.\cite{} page ??):
%\begin{align*}
%\Pr\Bigl[ p_{\samp_2}(B) \notin[3p(B)/4,4p(B)/3]\Bigr]
%&\leq
%\exp\Bigl( \frac{-({1}/{4})\cdot p(B)\cdot n}{2}\Bigr) + \exp\Bigl( \frac{-({1}/{3})\cdot p(B)\cdot n}{3}\Bigr)\\
%&\leq
%9/10,
%\end{align*}
%where in the last inequality we used that $p(B)\geq 2p_{\samp_1}(B)\geq 4k_0/n\geq 50/n$.
%\new{Can change $9/10$ to $1/10$,}





%
%Else, we have that $p(B) < p_{\samp_1}(B)/2$.
%We consider two cases:
%(i) if $10p(B) < 1/n$
%then
%\begin{align*}
%\Pr\Bigl[ p_{\samp_2}(B) \notin[3p(B)/4,4p(B)/3]\Bigr]
%&\leq
%\Pr\Bigl[ p_{\samp_2}(B) > 0 \Bigr]\\
%&\leq np(B) < 2/3 < 9/10.
%\end{align*}
%(ii) Else, if $(3/2)p(B) > 1/n$, then by the multiplicative Chernoff bound (see e.g.\cite{} page ??):
%\[
%\Pr\Bigl[ p_{\samp_2}(B) > (3/2)p(B)\Bigr]
%\leq
%\exp\Bigl( \frac{-({1}/{2})\cdot p(B)\cdot n}{3}\Bigr)\leq\exp(-1/9)\leq9/10.
%\]
%So, it follows that conditioned on $E_1$, 
%the event $E_2$ occurs with probability at least $1/10$.
%


%\end{proof}

%
%\begin{proof}[Proof of Lemma~\ref{lem:auxuc2}]\ 
%Instead of sampling $\samp_1$ and then $\samp_2$,
%we first sample $\samp=\samp_1\cup \samp_2$ and 
%then partition it to $\samp_1$ and $\samp_2$ uniformly.
%Now, given $\samp$ what is the probability (over the random partition)
%that $E_2$ occurs?
%
%Fix a sample $\samp$ of size $2n$
%Let $\B|_{\samp} = \{B|_{\samp} : B\in\B\}$.
%It suffices to show that the event
%\[E_2|_{\samp} = \text{"}\exists {B\in\B|_{\samp}}:
%  p_{\samp_1}(B)>2k/n \mbox{ and } p_{\samp_2}(B) \leq {3p_{\samp_1}(B)}/{4}
%\text{"}\]
%has probability at most $\delta/10$, for every every $\samp$ 
%(where the probability is over the partition of $\samp$ into $\samp_1,\samp_2$.
%To analyze this we use a union bound. 
%We only need to consider $B$'s in $\B|_{\samp}$ such that $k(B) > 2k$.
%Fix such a $B$.
%Without loss of generality assume that
%first $k(B)$ points out of the $2n$
%points in $S$ are in $B$. 
%For every $j\leq k(B)$,
%let $X_i$ denote the indicator of the event
%that the $i$'th point in $S$ was drawn into $S_2$.
%Set~$X=\sum_{i=1}^{k(B)}X_i$.
%Note that $B$ causes $E_2|_{\samp}$ to occur if and only if $X\leq 3k(B)/7$.
%To analyze this event define a random variable $Y=\sum_{i=1}^{k(B)}Y_i$,
%where the $Y_i$'s are independent Bernoulli random variables with probability $1/2$.
%One can verify that $\Pr[X\geq 3k(B)/7]\leq \Pr[Y\geq 3k(B)/7]$,
%and therefore it suffices to analyze the latter, 
%which is a standard application of Chernoff bound:
%
%\begin{align*}
%\Pr\bigl[X\leq3k(B)/7\bigr]&\leq 
%\Pr\bigl[Y\leq3k(B)/7\bigr]\\
%&=\Pr\bigl[Y- k(B)/2 < k(B)/14\bigr]\leq
%\exp\bigl(-2(1/14)^2k(B)\bigr) < \exp(-k/50)
%\end{align*}
%(where the last inequality is because $k(B) > 2k$).
%By Sauer's Lemma
%the number of distinct restrictions $B|_S$ is at most $(2n)^d$, and therefore 
%\[\Pr[E_2]\leq (2n^d)\exp(-k/100).\]
%Setting $k\geq 50\bigl(d\log(2n) + \log(10/\delta)\bigr)$ yields
%that this probability is at most $\delta/10$.
%
%
%\end{proof}
%
%  
%  
%%
%%  Fix $B$, we have two cases: either $p(B) \leq k_0/n$ or
%%  $p(B)>k_0/n$. We bound the probability of each case using chernoff:
%%  \begin{itemize}
%%  \item $p(B) \leq k_0/n$ then
%%  \[
%%  P\left[ k_{\samp_1}(B) \geq 2 k_0 \right] \leq \exp(-k_0/3)
%%  \]
%%  \item $p(B) > k_0/n$ then
%%  \[
%%  P\left[ k_{\samp_2}(B) < k_0/2 \right] \leq \exp(-k_0/2)
%%  \]
%%  \end{itemize}
%%
%%  We use the standard ghost sample argument to get a uniform bound
%%  over all $B$ and choose $k_0$ so that the bad probability is bound by $\delta$.
%
%  \end{proof}

\subsection{Uniform convergence of biases}

We will use the following Lemma, which we prove in the appendix, in Section~\ref{sec:auxuc}.
\begin{lemma}\label{lem:auxuc}
Let $\B$ be a family of specialists of VC dimension $d$.
Set $p_0 = \frac{100\bigl(d\log(2n) + \log(10/\delta)\bigr)}{n}$, 
where $1/2\geq \delta>0$,
and let $S=\bigl((x_i,y_i)\bigr) \sim p^n$.
Then:
\[
\Pr
\Bigl[
\exists {B\in\B}: p_{\samp}(B) \geq p_0 \mbox{ and } p(B) \leq \frac{p_{\samp}(B)}{10}
\Bigr]\leq \delta
\]
and
\[
\Pr
\Bigl[
\exists {B\in\B}: p(B) \geq p_0 \mbox{ and } p_{\samp}(B) \leq \frac{p(B)}{10}
\Bigr]\leq \delta.
\]


\end{lemma}

\begin{theorem}
Let $\B$ be a family of specialists of VC dimension $d$.
Set $k_0 = 100\bigl(d\log(2n) + \log(10/\frac{\delta}{2})\bigr)$, where
 $1/2\geq \delta>0$,
and let $S=\bigl((x_i,y_i)\bigr) \sim p^n$.
Then:
\[\Pr_{\samp\sim p^n}\left[\exists {B\in\B}:~\lvert
  \bias_{\samp}(B) -  \bias(B)\rvert \geq \sqrt{\frac{100d\log n +
      \log(1/\delta)}{k(B)} }
  \mbox{ and } k(B)>k_0
  \right] \leq \delta,
%\binom{n}{\leq d}\exp\bigl(-2\eps^2n\bigr),
\]
where $k(B) = \lvert\{ i : x_i\in B \}\rvert$.
%Then, the event:
%?For every specialist S: the absolute difference between its empirical bias and its true bias of S is at most sqrt{ (10d log n + log (1/delta) )/ k }?
%has probability at most delta.
\end{theorem}

\begin{proof}
We follow the double sampling argument.
Consider a double sample $S=(\samp_1,\samp_2)\sim p^{2n}$.
Let $E_1$ denote the event whose probability we want to bound:
\[
E_1 = \text{"}\exists {B\in\B}:~\lvert \bias_{\samp_1}(B) -
\bias(B)\rvert \geq \sqrt{\frac{100d\log n + \log(1/\delta)}{k_1(B)}}
\mbox{ and } k_1(B)>k_0
\text{"}, 
\]
and let $E_2$ denote the event:
\[
E_2 = \text{"}\exists {B\in\B}:~\lvert \bias_{\samp_1}(B) -  \bias_{\samp_2}(B)\rvert \geq \sqrt{\frac{100d\log n + \log(1/\delta)}{k_1(B)}}
  \mbox{ and }~ \bigl(k_1(B) + k_2(B)\bigr)>k_0
  \text{"},
\]
where $k_i(b)$ is the number of examples in $S_i$ that belong to $B$.

Now, assume towards contradiction that $\Pr[E_1] > \delta$.
The following two lemmas yield a contradiction.
\begin{lemma}\label{lem:aux1}
If $\Pr[E_1] > \delta$ then $\Pr[E_1]\leq 10\Pr[E_2].$
\end{lemma}
\begin{lemma}\label{lem:aux2}
$\Pr[E_2]\leq \delta/10.$
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:aux1}.]

\new{It suffices to show that $\Pr[E_2 \vert E_1]\geq \new{1/4}$.
Indeed, this would imply that $\Pr[E_2 \vert E_1]\geq \new{1/4}$, 
namely that $\Pr[E_1] \leq \new{4}\Pr[E_1 \land E_2]\leq 4\Pr[E_2]$.
We do it in two steps:
Consider the event
\[
F = \text{"}\forall {B\in\B}:~k_1(B) \geq k_0 \implies k_2(B) \geq k_1(B)/\new{20}
  \text{"}.
\]
We first prove that $\Pr[F \vert E_1] \geq \new{1/2}$ and then that $\Pr[E_2 \vert E_1\cap F]\geq 1/2$.
This concludes the proof as $\Pr[E_2 \vert E_1] = \Pr[F \vert E_1]\cdot\Pr[E_2 \vert E_1\cap F]$.}
\new{
We begin with the first step.
Indeed, Lemma~\ref{lem:auxuc} and the choice of $k_0$ 
imply that $\Pr[\lnot F] < \delta/2$.
Therefore, $\Pr[\lnot F \vert E_1] \leq \Pr[\lnot F]/\Pr[E_1] \leq (\delta/2)/\delta\leq 1/2$
(note that we used here the assumption that $\Pr[E_1]\geq \delta$).}

\new{Ignore what is written below}

For the second step,
let $(S_1,S_2)\in E_1\cap F$. 
Since the labels of $S_2$, denoted by $y(S_2)$ are independent with $E_1\cap F$
it suffices to show that 
\[\Pr_{y(S_2)}\bigl[(S_1,S_2)\in E_2 \bigr] \geq 1/2.\]
Let $B\in\B$ such that $\lvert \bias_{\samp_1}(B) -  \bias(B)\rvert \geq \sqrt{\frac{100d\log n + \log(1/\delta)}{k_1(B)} }$
and $k_1(B) > 2k_0$. 
We next analyze $\bias_{\samp_2}(B)$.
Standard concentration bound (e.g.\ Chernoff)
imply that with probability at least~$1/2$:
\[\lvert \bias_{\samp_2}(B) -  \bias(B)\bigr\rvert \leq \sqrt{\frac{10}{k_2(B)}} \leq \sqrt{\frac{\new{1000}}{k_1(B)}}.\]
Therefore:
\[\lvert \bias_{\samp_2}(B) -  \bias_{\samp_1}(B)\bigr\rvert \geq \sqrt{\frac{100d\log n + \log(1/\delta)}{k_1(B)} }- \sqrt{\frac{\new{1000}}{k_1(B)}},\]
which implies $E_2$.
Thus, the probability of $E_2$ conditioned on $E_1\cap F$
is at least $1/2$, which finishes the proof.

%
%Conditioning on this event, Chernoff bound implies that:
%\[
%\Pr\Bigl[\bigl\lvert \bias_{\samp_2}(B) -  \bias(B)\bigr\rvert \geq \sqrt{\frac{100d\log n + \log(1/\delta)}{k_2(B)} }\Bigr]
%\leq
%2\exp\bigl(\bigr)
%\]

\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:aux2}]\ \\
\paragraph{High level.}
Instead of sampling $S_1$ and then $S_2$,
we first sample $S=S_1\cup S_2$ and 
then partition it to $S_1$ and $S_2$ uniformly.
Now, given a sample $S$ what is the probability (over the random partition)
that $E_2$ occurs?
First, we only need to worry about $B$'s such that $k_S(B) > 2k_0$.
For such a $B$, the probability that the biases differ should be sufficiently
small such that when we union bound over all $O(n^d)$ different $B$'s
we still get that the resulting probability is at most $\delta$.

\end{proof}

%We distinguish between two cases: $p(B)$ is large and $p(B)$ is small.
%The case when $p(B)$ is sufficiently large (at least $1/n$) is fine (I think).
%
%On the other hand, if $p(B)<k_0/n$ then, with hight probability
%$k_1(B)<2k_0$ and the ball will be eliminated from consideration.

\iffalse
Further assume that $\bias(B) = 0.8$,  that $\bias_{\samp_1}(B)=0$, and that $k_1(B)$
is sufficiently large so the $\lvert \bias_{\samp_1}(B) -  \bias(B)\rvert \geq \sqrt{\frac{100d\log n + \log(1/\delta)}{k_1(B)} }$.
Now, since $p(B) \leq 1/1000n$ it follows that $k_2(B)=0$ with probability at least $0.99$.
Now, it is plausible to define $\bias_{\samp_2}(B)=0$ when $k_2(B)=0$,
and therefore we get that $\bias_{\samp_2}=\bias_{\samp_1}=0$ with probability at least $0.99$ and so the event $E_2$ occurs with probability less than $0.01$,
unlike what we wanted.
\fi
\end{proof}

\section{The algoithm}

\subsection{Defining safe sets}

Given a set of constraints, and given a polarity  $s \in
\{-1,+1\}$ we define a point $\x$ as a $s$-safe if
the following holds
\begin{itemize}
\item There is an $s$ constraint that contains $\x$.
\item For any $-s$ constraint $C$ that contains $\x$, there exists an
  $s$ constraint $D$ such that $D \subset C$. 
\end{itemize}

Points that are neither positive nor negative safe are called
``unsafe''

\subsection{Active Learning}

At each stage of Active Learning we query the label of two sets, each
of size $n$
\begin{itemize}
\item {\bf Uniform Set :} select points uniformly at random from the whole
  domain.
\item {\bf Active Set :} select points uniformaly at random from the unsafe
  set.
\end{itemize}

The Uniform sets from different stages are combined to form one
large label set. The active set from each stage is considered
separately.

The bias of each constraint is calculated with respect to the
cumulative uniform sample, and with respect to each one of the active
sets. Each of these sets defines a contraint on a subset of the $z_i$'s 

\section{Sufficient conditions on the true bias}
\newcommand{\Bayes}{f^*}
The goal of our algorithm is to generate a rule $f$ that
approximates the Bayes decision rule $\Bayes(\x) = \sign(\bias(\x))$.
Specifically, we set our goal to be minimizing the regret $\err(f) - \err(\Bayes)$.

In this section we define conditions on $\bias(x)$ that guarantee that
the algorithm will achieve a regret of $\epsilon$ after querying the
labels of $O(\log 1/\epsilon)$ instances.

We do this in two steps. First, we describe a technical condition on
$\bias(\x)$. Second, we give two more natural conditions that
guarantee the technical condition.

\subsection{$\epsilon$ approximation using Balls}

Let $\D$ be the distribution over $\X\times \{-1,+1\}$ where the
distribution over $\X$ is uniform and the conditional distribution
over the label $y$ given $\x$ is define by $\bias(\x)$.

\newcommand{\Be}{{\cal B}_{\epsilon}}
\newcommand{\fBe}{f_{\Be}}
\newcommand{\UBe}{U_{\epsilon}}
We say that $\D$ is $\epsilon$ approximable using balls for $\epsilon>0$ if
there exists a collection of balls $\Be$ such that 
\begin{itemize}
\item For all $B \in \Be$, $P_{\D}(B)>\epsilon$
\item For all $B \in \Be$, $|\bias(B)-1/2| > \epsilon$
\item For all $B \in \Be$ and for any point $\x \in B$, $\sign(\bias(\x))=\sign(\bias(B))$
\item %Let $\UBe$ be the union of all of the balls in $\Be$, i.e. $\UBe
  %\doteq \bigcup_{B \in \Be} B$\\
  Define the prediction function $\fBe$ as follows:
  \[
  \fBe(\x) \doteq \begin{cases}
    1 & \exists B \in \Be, \;\;\mbox{such that}\;\; \x \in B,
    \bias(B)>0 \\
    0 & \nexists B \in \Be \;\;\mbox{such that}\;\; \x \in B \\
    -1 & \exists B \in \Be, \;\;\mbox{such that}\;\; \x \in B,
    \bias(B)<0
  \end{cases}
  \]
  The final requirement is that $\err(\fBe) \leq \err(\Bayes)+\epsilon$
\end{itemize}

\subsection{Specific conditions}

I believe either of the following conditions implies
$\epsilon$-approximation using balls.

\begin{enumerate}
\item
  {\bf The conditional probability is Lipshitz smooth}. In other words, for
any $\x,\y \in \X$:
\[
|\bias(\x) - \bias(\y)| \leq \alpha \|\x-\y\|_2^{\beta}
\]
\item
The bias is bounded away from zero and {\bf the boundary is low
dimensional}. More technically:
\begin{itemize}
\item There exists some $\epsilon_0$ such that $\forall \x \in \X$, $|\bias(\x)|>\epsilon_0$.
\item Recall that $\X \subset R^d$. Define boundary balls as balls
  that contain and element with positive bias and an element with
  negative bias. Let $G_{\epsilon}$ be the union of all boundary balls
  with probability at most $\epsilon$. We say that the boundary has
  low dimension if $P_{\D}(G_{\epsilon}) \to 0$ as $\epsilon \to 0$
  (one has to be a bit careful in the definition here because $\X$ is finite, so
  $G_{1/2|\X|}=\emptyset$ trivially).  
\end{itemize}
\end{enumerate}


\appendix

\section{Proof of Lemma~\ref{lem:auxuc}}\label{sec:auxuc}
\begin{proof}
We use the standard uniform convergence bound with
the difference that we use the multiplicative Chernoff bound instead
of the additive bound. 
We follow the double sampling argument.
Consider a double sample $S=(\samp_1,\samp_2)\sim p^{2n}$.
Let $E_1$ denote the event whose probability we want to bound:
\[
E_1 = \text{"}\exists {B\in\B}:
  k_1(B) \geq t_0 \mbox{ and } p(B) \leq \frac{k_1(B)}{4n}
\text{"}, 
\]
and let $E_2$ denote the event:
\[
E_2 = \text{"}\exists {B\in\B}:
  k_1(B) \geq t_0 \mbox{ and }k_2(B) \leq \frac{k_1(B)}{2}
\text{"}.
\]
The proof follows from the following two lemmas:
\begin{lemma}\label{lem:auxuc1}
\[\Pr[E_1]\leq 10\Pr[E_2].\]
\end{lemma}
\begin{lemma}\label{lem:auxuc2}
\[\Pr[E_2]\leq \delta/10.\]
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lem:auxuc1}]
It suffices to show that $\Pr[E_2 \vert E_1]\geq 1/10$.
Indeed, this would imply that 
$\Pr[E_1] \leq 10\Pr[E_1 \land E_2]\leq 10\Pr[E_2]$.

Let $S_1\in E_1$. Since $S_2$ and $S_1$ are independent,
it suffices to show that 
\[\Pr_{S_2\sim p^n}\bigl[(S_1,S_2)\in E_2\bigr] \geq 1/10.\]
Let $B\in\B$ such that $p(B)\leq \frac{k_1(B)}{4n}$.
We want to show that $k_2(B)\leq \frac{k_1(B)}{2}$ with probability at least $1/10$.
We consider two cases:
(i) if $p(B) < 1/2n$
then we bound as follows:
\begin{align*}
\Pr\Bigl[ k_2(B) > \frac{k_1(B)}{n}\Bigr]
&\leq
\Pr\Bigl[ k_2(B) > 0 \Bigr]\\
&\leq np(B) < 1/2 < 9/10.
\end{align*}
(ii) Else, if $p(B) \geq 1/2n$, then by the multiplicative Chernoff bound (see e.g.\cite{} page ??):
\[
\Pr\Bigl[ k_2(B) > \frac{k_1(B)}{2}\Bigr]
\leq
\Pr\Bigl[ p_{\samp_2}(B) > 2p(B)\Bigr]
\leq
\exp\Bigl( \frac{-p(B)\cdot n}{3}\Bigr)\leq \exp(-1/6)\leq 9/10.
\]
So, conditioned on $E_1$, 
the event $E_2$ occurs with probability at least $1/10$.


\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:auxuc2}]

Instead of sampling $\samp_1$ and then $\samp_2$,
we first sample $\samp=\samp_1\cup \samp_2$ and 
then partition it to $\samp_1$ and $\samp_2$ uniformly.
Now, for a fixed $\samp$ what is the probability (over the random partition)
that $E_2$ occurs?
Let $\B|_{\samp} = \{B|_{\samp} : B\in\B\}$.
It suffices to show that the event
\[E_2|_{\samp} = \text{"}\exists {B\in\B|_{\samp}}:
  k_1(B)>t_0 \mbox{ and } k_2(B) \leq \frac{k_1(B)}{2}
\text{"}\]
has probability at most $\delta/10$, for every every $\samp$ 
(where the probability is over the partition of $\samp$ into $\samp_1,\samp_2$).
To analyze this we use a union bound. 
We only need to consider $B$'s in $\B|_{\samp}$ such that $k(B) > t_0$.
Fix such a $B$.
Without loss of generality assume that
first $k(B)$ points out of the $2n$
points in $S$ are in $B$. 
For every $i\leq k(B)$,
let $X_i$ denote the indicator of the event
that the $i$'th point in $S$ was drawn into $S_2$.
Set~$X=\sum_{i=1}^{k(B)}X_i$.
Note that $B$ causes $E_2|_{\samp}$ to occur if and only if $X\leq k(B)/3$.
To analyze this event define a random variable $Y=\sum_{i=1}^{k(B)}Y_i$,
where the $Y_i$'s are independent Bernoulli random variables with probability $1/2$.
One can verify that $\Pr[X\geq k(B)/3]\leq \Pr[Y\geq k(B)/3]$,
and therefore it suffices to analyze the latter, 
which is a standard application of Chernoff bound:
\begin{align*}
\Pr\bigl[X\leq k(B)/3\bigr]&\leq 
\Pr\bigl[Y\leq k(B)/3\bigr]\\
&=\Pr\bigl[Y- k(B)/2 < -k(B)/6\bigr]\leq
\exp\bigl(-2(1/6)^2k(B)\bigr) < \exp(-k/100)
\end{align*}
(where the last inequality is because $k(B) > t_0$).
By Sauer's Lemma
the number of distinct restrictions $B|_S$ is at most $(2n)^d$, and therefore 
\[\Pr[E_2]\leq (2n^d)\exp(-k/100).\]
Setting $k\geq 100\bigl(d\log(2n) + \log(10/\delta)\bigr)$ yields
that this probability is at most $\delta/10$.
\end{proof}
\end{proof}




\end{document}
